{"cells":[{"cell_type":"markdown","metadata":{"id":"-pfA1XNq2RnI"},"source":["# 3D Data Processing\n","\n","---\n","A.A. 2021/22 (6 CFU) - Dr. Daniel Fusaro\n","---\n","\n","\n","##3DP Lab4 - Point Cloud Segmentation - <b>INDIVIDUAL LABORATORY ASSIGNMENT</b>\n","\n","original paper -\n","[PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation](https://web.stanford.edu/~rqi/pointnet/)\n","\n","dataset link: https://drive.google.com/drive/folders/1_xPLa_rMIT3ggSSnp1W5mB74mojlWqvH?usp=sharing\n","\n","To add a link to the dataset in your Google Drive main folder, you need to:\n","\n","\n","1.   Click on the link\n","2.   Right click on \"dataset\"\n","3.   Click Add shortcut to Drive\n","\n","When you will mount your drive folder in Colab you will find this folder without the need of re-uploading it.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FvA7VTaKPP03"},"source":["## Segment what?\n","\n","In this Laboratory you will segment a point cloud taken from the famous [Semantic-Kitti dataset](http://semantic-kitti.org/) using PointNet.\n","\n","The original dataset counts about 30 labels (see next), but you will remap them to only 3:\n","\n","\n","*   Traversable (road, parking, sidewalk, ecc.)\n","*   Not-Traversable (cars, trucks, fences, trees, people, objects)\n","*   Unknown (outliers)\n","\n","The remap process is done using a key-value dictionary that maps an original label to the correspondent reduced label set.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"B6D7f56lGQYP"},"source":["# Install/import required packages"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3183,"status":"ok","timestamp":1655801818906,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"hc7eaUqkcmAz","outputId":"64b31c9b-5067-48cf-b04f-ef9c89658196"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: open3d in /usr/local/lib/python3.7/dist-packages (0.15.2)\n","Requirement already satisfied: wheel>=0.36.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (0.37.1)\n","Requirement already satisfied: pygments>=2.7.4 in /usr/local/lib/python3.7/dist-packages (from open3d) (2.12.0)\n","Requirement already satisfied: pyquaternion in /usr/local/lib/python3.7/dist-packages (from open3d) (0.9.9)\n","Requirement already satisfied: pillow>=8.2.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (9.0.0)\n","Requirement already satisfied: addict in /usr/local/lib/python3.7/dist-packages (from open3d) (2.4.0)\n","Requirement already satisfied: numpy>1.15 in /usr/local/lib/python3.7/dist-packages (from open3d) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from open3d) (6.0)\n","Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.7/dist-packages (from open3d) (3.2.2)\n","Requirement already satisfied: ipywidgets>=7.6.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (7.7.0)\n","Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (62.6.0)\n","Requirement already satisfied: jupyter-packaging~=0.10 in /usr/local/lib/python3.7/dist-packages (from open3d) (0.12.2)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.7/dist-packages (from open3d) (1.0.2)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (1.3.5)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from open3d) (4.64.0)\n","Requirement already satisfied: jupyterlab==3.*,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (3.4.3)\n","Requirement already satisfied: nbclassic~=0.2 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (0.3.7)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (5.5.0)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (4.10.0)\n","Requirement already satisfied: tornado>=6.1.0 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (6.1)\n","Requirement already satisfied: jinja2>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (3.1.2)\n","Requirement already satisfied: jupyterlab-server~=2.10 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (2.14.0)\n","Requirement already satisfied: jupyter-server~=1.16 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (1.17.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (21.3)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (1.1.0)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (0.2.0)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (5.3.0)\n","Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (5.4.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (3.6.0)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (4.10.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.0->open3d) (7.3.4)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (4.4.2)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (0.8.1)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (1.0.18)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (4.8.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (0.7.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.1->jupyterlab==3.*,>=3.0.0->open3d) (2.0.1)\n","Requirement already satisfied: deprecation in /usr/local/lib/python3.7/dist-packages (from jupyter-packaging~=0.10->open3d) (2.1.0)\n","Requirement already satisfied: tomlkit in /usr/local/lib/python3.7/dist-packages (from jupyter-packaging~=0.10->open3d) (0.11.0)\n","Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (21.3.0)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (23.1.0)\n","Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.13.3)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.8.0)\n","Requirement already satisfied: nbconvert>=6.4.4 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (6.5.0)\n","Requirement already satisfied: prometheus-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.14.1)\n","Requirement already satisfied: websocket-client in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.3.3)\n","Requirement already satisfied: anyio<4,>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (3.6.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (4.1.1)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.2.0)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (2.10)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.6.0->open3d) (0.4)\n","Requirement already satisfied: nest-asyncio>=1.5.4 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.6.0->open3d) (1.5.5)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.6.0->open3d) (2.8.2)\n","Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (4.11.4)\n","Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (4.3.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (2.23.0)\n","Requirement already satisfied: json5 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (0.9.8)\n","Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (2.10.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (3.8.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (0.18.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (5.7.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (21.4.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->open3d) (1.4.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->open3d) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->open3d) (3.0.9)\n","Requirement already satisfied: notebook-shim>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from nbclassic~=0.2->jupyterlab==3.*,>=3.0.0->open3d) (0.1.0)\n","Requirement already satisfied: notebook<7 in /usr/local/lib/python3.7/dist-packages (from nbclassic~=0.2->jupyterlab==3.*,>=3.0.0->open3d) (5.3.1)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.5.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (4.6.3)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.8.4)\n","Requirement already satisfied: tinycss2 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.1.1)\n","Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.2.2)\n","Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.6.4)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (5.0.0)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.7.1)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=4.2.0->ipywidgets>=7.6.0->open3d) (2.15.3)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->open3d) (2022.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->jupyterlab==3.*,>=3.0.0->open3d) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->jupyterlab==3.*,>=3.0.0->open3d) (1.15.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->open3d) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->open3d) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->open3d) (1.1.0)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.3->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.7.0)\n","Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.7/dist-packages (from argon2-cffi->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (21.2.0)\n","Requirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.15.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (2.21)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.5.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (2022.6.15)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (1.24.3)\n"]}],"source":["# useful for visualization\n","## note: it's not necessary to restart the notebook environment after installation\n","!pip install open3d"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3442,"status":"ok","timestamp":1655801822343,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"VY952dUzNi-Q","outputId":"4015c2be-aab9-426c-b733-8ce1f307e1ed"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: Pillow==9.0.0 in /usr/local/lib/python3.7/dist-packages (9.0.0)\n"]}],"source":["# Google Colab pyTorch needs this Pillow version\n","## note: it's not necessary to restart the notebook environment after installation\n","!pip install Pillow==9.0.0"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4529,"status":"ok","timestamp":1655801826868,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"qSyKEXWMmI2Y"},"outputs":[],"source":["import numpy as np\n","import random\n","import math\n","import time\n","import struct\n","import os\n","\n","# pyTorch imports\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","# a nice training progress bar\n","from tqdm import tqdm\n","\n","# visualization\n","import open3d as o3d\n","import plotly.graph_objects as go\n"]},{"cell_type":"markdown","metadata":{"id":"DuR9palyGqil"},"source":["# Connect and mount your Google Drive"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22589,"status":"ok","timestamp":1655801849451,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"Wjgl1d34-CWw","outputId":"c510a489-e808-4491-b61b-4a93f105c14e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive_path = '/content/drive'\n","drive.mount(drive_path)"]},{"cell_type":"markdown","metadata":{"id":"k_HKaOUmy0fS"},"source":["# Select the first GPU (if available)"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1655801849451,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"vIlTUpNPyBdK","outputId":"be800aed-6dbc-4582-8bed-1ebc88cb299b"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda:0\n"]}],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"hK1CJSoJGvi7"},"source":["# General Parameters"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655801849452,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"1LWQSdhp3CiT"},"outputs":[],"source":["numpoints = 20000 # [number of points]\n","max_dist = 15     # [meters]\n","min_dist = 4      # [meters]\n","\n","# transform distances to squares (code optimization)\n","max_dist *= max_dist\n","min_dist *= min_dist\n","\n","size_float = 4\n","size_small_int = 2\n","\n","dataset_path = os.path.join(drive_path, \"MyDrive\", \"dataset\")"]},{"cell_type":"markdown","metadata":{"id":"BJYBour70ZAf"},"source":["# Read Data utilities"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655801849452,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"Zbj0fKpTpWtJ"},"outputs":[],"source":["def sample(pointcloud, labels, numpoints_to_sample):\n","  \"\"\"\n","    INPUT\n","        pointcloud          : list of 3D points\n","        labels              : list of integer labels\n","        numpoints_to_sample : number of points to sample\n","  \"\"\"\n","  tensor = np.concatenate((pointcloud, np.reshape(labels, (labels.shape[0], 1))), axis= 1)\n","  tensor = np.asarray(random.choices(tensor, weights=None, cum_weights=None, k=numpoints_to_sample))\n","  pointcloud_ = tensor[:, 0:3]\n","  labels_ = tensor[:, 3]\n","  labels_ = np.array(labels_, dtype=np.int_)\n","  return pointcloud_, labels_"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655801849453,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"DYTnoM6amdYg"},"outputs":[],"source":["def readpc(pcpath, labelpath, reduced_labels=True):\n","  \"\"\"\n","    INPUT\n","        pcpath         : path to the point cloud \".bin\" file\n","        labelpath      : path to the labels \".label\" file\n","        reduced_labels : flag to select which label encoding to return\n","                        [True]  -> values in range [0, 1, 2]   -- default\n","                        [False] -> all Semantic-Kitti dataset original labels\n","  \"\"\"\n","\n","  pointcloud, labels = [], []\n","\n","  with open(pcpath, \"rb\") as pc_file, open(labelpath, \"rb\") as label_file:\n","    byte = pc_file.read(size_float*4)\n","    label_byte = label_file.read(size_small_int)\n","    _ = label_file.read(size_small_int)\n","\n","    while byte:\n","      x,y,z, _ = struct.unpack(\"ffff\", byte)      # unpack 4 float values\n","      label = struct.unpack(\"H\", label_byte)[0]   # unpach 1 Unsigned Short value\n","      \n","      d = x*x + y*y + z*z       # Euclidean norm\n","\n","      if min_dist<d<max_dist:\n","          pointcloud.append([x, y, z])\n","          if reduced_labels:            # for reduced labels range\n","            labels.append(label_remap[label])\n","          else:                         # for full labels range\n","            labels.append(label)\n","      \n","      byte = pc_file.read(size_float*4)\n","      label_byte = label_file.read(size_small_int)\n","      _ = label_file.read(size_small_int)\n","  \n","\n","  pointcloud  = np.array(pointcloud)\n","  labels      = np.array(labels)\n","\n","  # return fixed_sized lists of points/labels (fixed size: numpoints)\n","  return sample(pointcloud, labels, numpoints)\n"]},{"cell_type":"markdown","metadata":{"id":"VWcO-JoNzH17"},"source":["# Data visualization"]},{"cell_type":"markdown","metadata":{"id":"vG29l-nHkW8g"},"source":["## Define Color Maps for visualization\n","\n","Let's define some color mapping to associate an integer value to an RGB color scheme\n","\n","\n","\n","*   **semantic_kitti_color_scheme**: original Semantic-Kitti color scheme (see [here](https://github.com/PRBonn/semantic-kitti-api/blob/master/config/semantic-kitti-all.yaml))\n","*   **label_remap**: remapping of Semantic-Kitti labels to \"*Unknown*\", \"*Traversable*\", \"*Not-Traversable*\"\n","*   **remap_color_scheme**: color scheme for rendering these 3 labels\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655801849453,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"TT5QH2QPkV8_"},"outputs":[],"source":["semantic_kitti_color_scheme = {\n","  0 : [0, 0, 0],        # \"unlabeled\"\n","  1 : [0, 0, 255],      # \"outlier\"\n","  10: [245, 150, 100],  # \"car\"\n","  11: [245, 230, 100],  # \"bicycle\"\n","  13: [250, 80, 100],   # \"bus\"\n","  15: [150, 60, 30],    # \"motorcycle\"\n","  16: [255, 0, 0],      # \"on-rails\"\n","  18: [180, 30, 80],    # \"truck\"\n","  20: [255, 0, 0],      # \"other-vehicle\"\n","  30: [30, 30, 255],    # \"person\"\n","  31: [200, 40, 255],   # \"bicyclist\"\n","  32: [90, 30, 150],    # \"motorcyclist\"\n","  40: [255, 0, 255],    # \"road\"\n","  44: [255, 150, 255],  # \"parking\"\n","  48: [75, 0, 75],      # \"sidewalk\"\n","  49: [75, 0, 175],     # \"other-ground\"\n","  50: [0, 200, 255],    # \"building\"\n","  51: [50, 120, 255],   # \"fence\"\n","  52: [0, 150, 255],    # \"other-structure\"\n","  60: [170, 255, 150],  # \"lane-marking\"\n","  70: [0, 175, 0],      # \"vegetation\"\n","  71: [0, 60, 135],     # \"trunk\"\n","  72: [80, 240, 150],   # \"terrain\"\n","  80: [150, 240, 255],  # \"pole\"\n","  81: [0, 0, 255],      # \"traffic-sign\"\n","  99: [255, 255, 50],   # \"other-object\"\n","  252: [245, 150, 100], # \"moving-car\"\n","  253: [200, 40, 255],  # \"moving-bicyclist\"\n","  254: [30, 30, 255],   # \"moving-person\"\n","  255: [90, 30, 150],   # \"moving-motorcyclist\"\n","  256: [255, 0, 0],     # \"moving-on-rails\"\n","  257: [250, 80, 100],  # \"moving-bus\"\n","  258: [180, 30, 80],   # \"moving-truck\"\n","  259: [255, 0, 0],     # \"moving-other-vehicle\"\n","}"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655801849453,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"WxPnGnZ9EZVZ"},"outputs":[],"source":["label_remap = {\n","  0 :  0, # \"unlabeled\"\n","  1 :  0, # \"outlier\"\n","  10:  2, # \"car\"\n","  11:  2, # \"bicycle\"\n","  13:  2, # \"bus\"\n","  15:  2, # \"motorcycle\"\n","  16:  2, # \"on-rails\"\n","  18:  2, # \"truck\"\n","  20:  2, # \"other-vehicle\"\n","  30:  2, # \"person\"\n","  31:  2, # \"bicyclist\"\n","  32:  2, # \"motorcyclist\"\n","  40:  1, # \"road\"\n","  44:  1, # \"parking\"\n","  48:  1, # \"sidewalk\"\n","  49:  1, # \"other-ground\"\n","  50:  2, # \"building\"\n","  51:  2, # \"fence\"\n","  52:  2, # \"other-structure\"\n","  60:  1, # \"lane-marking\"\n","  70:  2, # \"vegetation\"\n","  71:  2, # \"trunk\"\n","  72:  2, # \"terrain\"\n","  80:  2, # \"pole\"\n","  81:  2, # \"traffic-sign\"\n","  99:  2, # \"other-object\"\n","  252: 2, # \"moving-car\"\n","  253: 2, # \"moving-bicyclist\"\n","  254: 2, # \"moving-person\"\n","  255: 2, # \"moving-motorcyclist\"\n","  256: 2, # \"moving-on-rails\"\n","  257: 2, # \"moving-bus\"\n","  258: 2, # \"moving-truck\"\n","  259: 2, # \"moving-other-vehicle\"\n","}"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655801849454,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"qbHnYcqPHkZq"},"outputs":[],"source":["remap_color_scheme = [\n","  [0, 0, 0],\n","  [0, 255, 0],\n","  [0, 0, 255]\n","]"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655801849454,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"AiMzHXD_ANLh"},"outputs":[],"source":["def remap_to_bgr(integer_labels, color_scheme):\n","  bgr_labels = []\n","  for n in integer_labels:\n","    bgr_labels.append(color_scheme[int(n)][::-1])\n","  np_bgr_labels = np.array(bgr_labels)\n","  return np_bgr_labels"]},{"cell_type":"markdown","metadata":{"id":"xo0vc3C8IJXA"},"source":["## Visualization utilities\n","\n","In order to visualize colored point clouds we make use of the Python package *Open3D*.\n","\n","Unfortunately, the original doesn't run on Colab.\n","So, we replace the drawing function with a custom one (*draw_geometries*) that allows the rendering.\n","\n"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655801849454,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"9ceZKVwpfdG0"},"outputs":[],"source":["def draw_geometries(geometries):\n","    graph_objects = []\n","\n","    for geometry in geometries:\n","        geometry_type = geometry.get_geometry_type()\n","        \n","        if geometry_type == o3d.geometry.Geometry.Type.PointCloud:\n","            points = np.asarray(geometry.points)\n","            colors = None\n","            if geometry.has_colors():\n","                colors = np.asarray(geometry.colors)\n","            elif geometry.has_normals():\n","                colors = (0.5, 0.5, 0.5) + np.asarray(geometry.normals) * 0.5\n","            else:\n","                geometry.paint_uniform_color((1.0, 0.0, 0.0))\n","                colors = np.asarray(geometry.colors)\n","\n","            scatter_3d = go.Scatter3d(x=points[:,0], y=points[:,1], z=points[:,2], mode='markers', marker=dict(size=1, color=colors))\n","            graph_objects.append(scatter_3d)\n","\n","        if geometry_type == o3d.geometry.Geometry.Type.TriangleMesh:\n","            triangles = np.asarray(geometry.triangles)\n","            vertices = np.asarray(geometry.vertices)\n","            colors = None\n","            if geometry.has_triangle_normals():\n","                colors = (0.5, 0.5, 0.5) + np.asarray(geometry.triangle_normals) * 0.5\n","                colors = tuple(map(tuple, colors))\n","            else:\n","                colors = (1.0, 0.0, 0.0)\n","            \n","            mesh_3d = go.Mesh3d(x=vertices[:,0], y=vertices[:,1], z=vertices[:,2], i=triangles[:,0], j=triangles[:,1], k=triangles[:,2], facecolor=colors, opacity=0.50)\n","            graph_objects.append(mesh_3d)\n","        \n","    fig = go.Figure(\n","        data=graph_objects,\n","        layout=dict(\n","            scene=dict(\n","                xaxis=dict(visible=False),\n","                yaxis=dict(visible=False),\n","                zaxis=dict(visible=False),\n","                aspectmode='data'\n","            )\n","        )\n","    )\n","    fig.show()"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655801849455,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"PvGnESicn1B5"},"outputs":[],"source":["def visualize3DPointCloud(np_pointcloud, np_labels):\n","  \"\"\"\n","  INPUT\n","      np_pointcloud : numpy array of 3D points\n","      np_labels     : numpy array of integer labels\n","  \"\"\"\n","  assert(len(np_pointcloud) == len(np_labels))\n","\n","  \n","  pcd = o3d.geometry.PointCloud()\n","  v3d = o3d.utility.Vector3dVector\n","\n","  # set geometry point cloud points\n","  pcd.points = v3d(np_pointcloud)\n","  \n","  # scale color values in range [0:1]\n","  pcd.colors = o3d.utility.Vector3dVector(np_labels / 255.0)\n","\n","  # replace rendering function\n","  o3d.visualization.draw_geometries = draw_geometries\n","\n","  # visualize the colored point cloud\n","  o3d.visualization.draw_geometries([pcd])"]},{"cell_type":"markdown","metadata":{"id":"7aKjaHOdIWPc"},"source":["## Visualization Example\n","\n","Let's try to visualize an example point cloud"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1Lue7tL6pkoGj--pTZYYM9VBTMg8Hdz7r"},"executionInfo":{"elapsed":15663,"status":"ok","timestamp":1655801865109,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"wxlsg3I7dg75","outputId":"aff6fc57-b570-4ce5-dcaf-679dc3f966da"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["# define point cloud example index and absolute paths\n","pointcloud_index = 700\n","pcpath    = os.path.join(dataset_path, \"sequences\", \"00\", \"velodyne\", str(pointcloud_index).zfill(6) + \".bin\"  )\n","labelpath = os.path.join(dataset_path, \"sequences\", \"00\", \"labels\",   str(pointcloud_index).zfill(6) + \".label\")\n","\n","# load pointcloud and labels with original Semantic-Kitti labels\n","pointcloud, labels = readpc(pcpath, labelpath, False)\n","labels = remap_to_bgr(labels, semantic_kitti_color_scheme)\n","print(\"Semantic-Kitti original color scheme\")\n","visualize3DPointCloud(pointcloud, labels)\n","\n","# load pointcloud and labels with remapped labels\n","pointcloud, labels = readpc(pcpath, labelpath)\n","labels = remap_to_bgr(labels, remap_color_scheme)\n","print(\"Remapped color scheme\")\n","visualize3DPointCloud(pointcloud, labels)"]},{"cell_type":"markdown","metadata":{"id":"nfpcbTjCzKgQ"},"source":["## Data Transformation"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655801865109,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"vaS0JED6u2gC"},"outputs":[],"source":["class Normalize(object):\n","    def __call__(self, pointcloud):\n","        assert len(pointcloud.shape)==2\n","        \n","        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n","        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n","\n","        return  norm_pointcloud"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655801865110,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"m1jBl3JxwZA7"},"outputs":[],"source":["class ToTensor(object):\n","    def __call__(self, pointcloud):\n","        assert len(pointcloud.shape)==2\n","\n","        return torch.from_numpy(pointcloud)"]},{"cell_type":"code","execution_count":18,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655801865110,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"CUK7w-FpwaSE"},"outputs":[],"source":["def default_transforms():\n","    return transforms.Compose([\n","                                Normalize(),\n","                                ToTensor()\n","                              ])"]},{"cell_type":"markdown","metadata":{"id":"dLp113kezNBG"},"source":["# PointCloud Dataset\n","\n","`torch.utils.data.Dataset` is an abstract class representing a dataset. Your custom dataset should inherit `Dataset` and override the following methods:\n","\n","* `__init__` to initialize your dataset. For example, if your dataset fits in memory, you can load the entire dataset in a list, or you can just store the list of dataset files.\n","* `__len__` so that len(dataset) returns the size of the dataset.\n","* `__getitem__` to support indexing such that `dataset[i]` can be used to get  the i-th sample\n","\n","Therefore, the structure of the class is:\n","\n","```\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, init_parameters, transform=None):\n","        self.transform = transform\n","        [...]\n","\n","    def __len__(self):\n","        [...]\n","\n","    def __getitem__(self, idx):\n","        [...]\n","        if self.transform:\n","            sample = self.transform(sample)\n","\n","        return sample\n","```\n","\n","Typically, a `transform` function is provided during initialization. This function is applied to each sample at runtime, so it is executed every time you load a sample from the dataset. This is really helpful, for example, to add random data transformation during training, such as random image rotation, random noise..."]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655801865111,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"C2NuVvQCweUS"},"outputs":[],"source":["class PointCloudData(Dataset):\n","    def __init__(self, dataset_path, transform=default_transforms(), start=0, end=1000):\n","        \"\"\"\n","          INPUT\n","              dataset_path: path to the dataset folder\n","              transform   : transform function to apply to point cloud\n","              start       : index of the first file that belongs to dataset\n","              end         : index of the first file that do not belong to dataset\n","        \"\"\"\n","        self.dataset_path = dataset_path\n","        self.transforms = transform\n","\n","        self.pc_path = os.path.join(self.dataset_path, \"sequences\", \"00\", \"velodyne\")\n","        self.lb_path = os.path.join(self.dataset_path, \"sequences\", \"00\", \"labels\")\n","\n","        self.pc_paths = os.listdir(self.pc_path)\n","        self.lb_paths = os.listdir(self.lb_path)\n","        assert(len(self.pc_paths) == len(self.lb_paths))\n","\n","        self.start = start\n","        self.end   = end\n","\n","        # clip paths according to the start and end ranges provided in input\n","        self.pc_paths = self.pc_paths[start: end]\n","        self.lb_paths = self.lb_paths[start: end]\n","\n","    def __len__(self):\n","        return len(self.pc_paths)\n","\n","    def __getitem__(self, idx):\n","      item_name = str(idx + self.start).zfill(6)\n","      pcpath = os.path.join(self.pc_path, item_name + \".bin\")\n","      lbpath = os.path.join(self.lb_path, item_name + \".label\")\n","      \n","      # load points and labels\n","      pointcloud, labels = readpc(pcpath, lbpath)\n","\n","      # transform\n","      torch_pointcloud  = torch.from_numpy(pointcloud)\n","      torch_labels      = torch.from_numpy(labels)\n","\n","      return torch_pointcloud, torch_labels"]},{"cell_type":"markdown","metadata":{"id":"ABoR7ciUCrq_"},"source":["# Dataset Creation\n","\n","Now we can instantiate our training and test dataset objects."]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655801865111,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"mq_sC59txbQF"},"outputs":[],"source":["train_ds  = PointCloudData(dataset_path, start=0,   end=100)\n","val_ds    = PointCloudData(dataset_path, start=100, end=120)\n","test_ds   = PointCloudData(dataset_path, start=120, end=150)"]},{"cell_type":"markdown","metadata":{"id":"kC5c_b1quNFT"},"source":["Creating a `Dataset` class may seem unnecessary for the most basic problems. But it really helps when the dataset and the training procedure start to get more complex.\n","\n","One of the most useful benefit of defining a `Dataset` class is the possiblity to use the PyTorch `Dataloader` module.\n","\n","By operating on the dataset directly, we are losing out on a lot of features by using a simple for loop to iterate over the data. In particular, we are missing out on:\n","\n","* Batching the data\n","* Shuffling the data\n","* Load the data in parallel using multiprocessing workers.\n","\n","`torch.utils.data.DataLoader` is an iterator which provides all these features. Parameters used below should be clear."]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655801865111,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"lX2pNbMhxm0N"},"outputs":[],"source":["# warning: batch_size needs to be at least 2\n","train_loader  = DataLoader( dataset=train_ds,  batch_size=5, shuffle=True  )\n","val_loader    = DataLoader( dataset=val_ds,    batch_size=5, shuffle=False )\n","test_loader   = DataLoader( dataset=test_ds,   batch_size=1, shuffle=False )"]},{"cell_type":"markdown","metadata":{"id":"BPtnhEyuy9ks"},"source":["# Network Definition"]},{"cell_type":"markdown","metadata":{"id":"1HjszmGzub9M"},"source":["## Network Base Module\n","\n","A network is defined by extending the *torch.nn.module* class. The basic structure is:\n","\n","```\n","class Net(nn.Module):\n","    \n","    def __init__(self, input_parameters):\n","        super().__init__() # This executes the parent __init__ method\n","        [...]\n","\n","    def forward(self, x, optional_parameters):\n","        [...]\n","        return out # return the output of the network\n","```\n","\n","You need to define two methods:\n","*   **\\_\\_init\\_\\_**: The constructor method. This is exectuted when the object is initialized (no need to call it explicitly). Here you have to instantiate all the network's parameters. PyTorch provides utility functions to easily initialize most of the commonly used deep learning layers.\n","*   **forward**: Here you define the forward pass of the network, from the input *x* to the output (the method must return the network output). You just need to define the forward part, the back-propagation is automatically tracked by the framework!"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655801865111,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"HpMGFh5_bj5I"},"outputs":[],"source":["# Multi Layer Perceptron\n","class MLP(nn.Module):\n","   def __init__(self, input_size, output_size):\n","     super().__init__()\n","     self.input_size   = input_size\n","     self.output_size  = output_size\n","     self.conv  = nn.Conv1d(self.input_size, self.output_size, 1)\n","     self.bn    = nn.BatchNorm1d(self.output_size)\n","\n","   def forward(self, input):\n","     return F.relu(self.bn(self.conv(input)))\n","\n","# Fully Connected with Batch Normalization\n","class FC_BN(nn.Module):\n","   def __init__(self, input_size, output_size):\n","     super().__init__()\n","     self.input_size   = input_size\n","     self.output_size  = output_size\n","     self.lin  = nn.Linear(self.input_size, self.output_size)\n","     self.bn    = nn.BatchNorm1d(self.output_size)\n","\n","   def forward(self, input):\n","     return F.relu(self.bn(self.lin(input)))\n","\n","class TNet(nn.Module):\n","   def __init__(self, k=3):\n","      super().__init__()\n","      self.k=k\n","\n","      self.mlp1 = MLP(self.k, 64)\n","      self.mlp2 = MLP(64, 128)\n","      self.mlp3 = MLP(128, 1024)\n","\n","      self.fc_bn1 = FC_BN(1024, 512)\n","      self.fc_bn2 = FC_BN(512,256)\n","\n","      self.fc3 = nn.Linear(256,k*k)\n","    \n","\n","   def forward(self, input):\n","      # input.shape == (batch_size,n,3)\n","      \n","      bs = input.size(0)\n","      xb = self.mlp1(input)\n","      xb = self.mlp2(xb)\n","      xb = self.mlp3(xb)\n","\n","      pool = nn.MaxPool1d(xb.size(-1))(xb)\n","      flat = nn.Flatten(1)(pool)\n","\n","      xb = self.fc_bn1(flat)\n","      xb = self.fc_bn2(xb)\n","      \n","      # initialize as identity\n","      init = torch.eye(self.k, requires_grad=True).repeat(bs,1,1)\n","      if xb.is_cuda:\n","        init=init.cuda()\n","      matrix = self.fc3(xb).view(-1,self.k,self.k) + init\n","      return matrix"]},{"cell_type":"markdown","metadata":{"id":"ag_-z8VzvpDU"},"source":["### PointNet Module\n","\n","Here you need to complete the Network Module implementing **\\_\\_init\\_\\_** and **forward** methods.\n","\n","Refer to previous cells for a description of these methods and example of implementation.\n","\n","> **!! Please note:**\n","\n","*   layers input and output size parameters need to match with those mentioned in PointNet paper\n","*   use MLP and TNet modules to complete the code"]},{"cell_type":"code","execution_count":23,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655801865112,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"mEQ4Zx5WoLDx"},"outputs":[],"source":["class PointNet(nn.Module):\n","   def __init__(self):\n","        super().__init__()\n","        self.input_transform   = TNet(k=3)\n","        \n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        self.mlp1_1 = MLP(3, 64)\n","        self.mlp1_2 = MLP(64, 64)\n","\n","        self.feature_transform = TNet(k=64)\n","        \n","        self.mlp2_1 = MLP(64, 64)\n","        self.mlp2_2 = MLP(64, 128)\n","        self.mlp2_3 = MLP(128, 1024)\n","\n","        ###########################################################\n","\n","\n","   def forward(self, input):\n","        n_pts = input.size()[2]\n","        matrix3x3 = self.input_transform(input)\n","        input_transform_output = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n","\n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        xb = self.mlp1_1(input_transform_output)\n","        xb = self.mlp1_2(xb)\n","\n","        #FEATURE TRANSFORM PART\n","        #computation feature transform by using T-Net\n","        matrix64x64 = self.feature_transform(xb)\n","        #matrix multiplication among features and feature transform matrix\n","        feature_transform_output = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n","\n","        xb = self.mlp2_1(feature_transform_output)\n","        xb = self.mlp2_2(xb)\n","        xb = self.mlp2_3(xb) \n","\n","        pool = nn.MaxPool1d(xb.size(-1))(xb)\n","        global_feature = nn.Flatten(1)(pool)\n","        \n","        ###########################################################\n","\n","        global_feature_repeated = nn.Flatten(1)(global_feature).repeat(n_pts,1,1).transpose(0,2).transpose(0,1)\n","\n","        return [feature_transform_output, global_feature_repeated], matrix3x3, matrix64x64"]},{"cell_type":"markdown","metadata":{"id":"UZp83osYxP7O"},"source":["### PointNetSeg Module\n","\n","Also here you need to complete the Network Module implementing **\\_\\_init\\_\\_** and **forward** methods.\n","\n","Refer to previous cells for a description of these methods and example of implementation.\n","\n","> **!! Please note**\n","\n","*   layers input and output size parameters need to match with those mentioned in PointNet paper\n","*   use MLP module to complete the code"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655801865112,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"mjrDg4ZHvnBp"},"outputs":[],"source":["class PointNetSeg(nn.Module):\n","    def __init__(self, classes = 3):\n","        super().__init__()\n","        self.pointnet = PointNet()\n","\n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","        \n","        self.mlp1_1 = MLP(1088,512)\n","        self.mlp1_2 = MLP(512,256)\n","        self.mlp1_3 = MLP(256,128)\n","\n","        self.mlp2_1 = MLP(128,128)\n","        self.mlp2_2 = MLP(128,classes)\n","\n","        ###########################################################\n","\n","        self.logsoftmax = nn.LogSoftmax(dim=1)\n","        \n","\n","    def forward(self, input):\n","        inputs, matrix3x3, matrix64x64 = self.pointnet(input)\n","        stack = torch.cat(inputs,1)\n","        \n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        xb = self.mlp1_1(stack)\n","        xb = self.mlp1_2(xb)\n","        point_features = self.mlp1_3(xb)\n","        \n","        xb = self.mlp2_1(point_features)\n","        output = self.mlp2_2(xb)\n","\n","        ###########################################################\n","\n","        return self.logsoftmax(output), matrix3x3, matrix64x64"]},{"cell_type":"markdown","metadata":{"id":"ZnZqPJCixcKR"},"source":["# Loss Function\n","\n","This is the loss used by the model to update its weights during training loop.\n","\n","For details, please refer to the PointNet paper."]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655801865112,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"BNxoSQd-ao2W"},"outputs":[],"source":["def pointNetLoss(outputs, labels, m3x3, m64x64, alpha = 0.0001):\n","    criterion = torch.nn.NLLLoss()\n","    bs=outputs.size(0)\n","    id3x3 = torch.eye(3, requires_grad=True).repeat(bs,1,1)\n","    id64x64 = torch.eye(64, requires_grad=True).repeat(bs,1,1)\n","    if outputs.is_cuda:\n","        id3x3=id3x3.cuda()\n","        id64x64=id64x64.cuda()\n","    diff3x3 = id3x3-torch.bmm(m3x3,m3x3.transpose(1,2))\n","    diff64x64 = id64x64-torch.bmm(m64x64,m64x64.transpose(1,2))\n","    return criterion(outputs, labels) + alpha * (torch.norm(diff3x3)+torch.norm(diff64x64)) / float(bs)"]},{"cell_type":"markdown","metadata":{"id":"P38WBTtMyFeC"},"source":["# Training loop"]},{"cell_type":"code","execution_count":26,"metadata":{"executionInfo":{"elapsed":6730,"status":"ok","timestamp":1655801871834,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"uWSroT5TyDnt"},"outputs":[],"source":["pointnet = PointNetSeg()\n","pointnet.to(device);"]},{"cell_type":"code","execution_count":27,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1655801871834,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"wqwUjESLyKv3"},"outputs":[],"source":["optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.005)"]},{"cell_type":"code","execution_count":28,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1655801871835,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"CAf8UGcPazjx"},"outputs":[],"source":["def train(model, train_loader, val_loader=None,  epochs=15, save=True):\n","    best_val_acc = -1.0\n","    for epoch in range(epochs): \n","        pointnet.train()\n","        running_loss = 0.0\n","\n","        for i, data in enumerate(train_loader, 0):\n","            inputs, labels = data\n","            inputs = inputs.to(device).float()\n","            labels = labels.to(device)\n","            optimizer.zero_grad()\n","            outputs, m3x3, m64x64 = pointnet(inputs.transpose(1,2))\n","            loss = pointNetLoss(outputs, labels, m3x3, m64x64)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            if i % 10 == 9 or True:    # print every 10 mini-batches\n","                    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n","                    running_loss = 0.0\n","\n","        pointnet.eval()\n","        correct = total = 0\n","\n","        # validation\n","        with torch.no_grad():\n","            for data in val_loader:\n","                inputs, labels = data\n","                inputs = inputs.to(device).float()\n","                labels = labels.to(device)\n","                outputs, __, __ = pointnet(inputs.transpose(1,2))\n","                _, predicted = torch.max(outputs.data, 1)\n","                \n","                total   += labels.size(0) * labels.size(1)\n","                correct += (predicted == labels).sum().item()\n","\n","        print(\"correct\", correct, \"/\", total)\n","        val_acc = 100.0 * correct / total\n","        print('Valid accuracy: %d %%' % val_acc)\n","\n","        # save the model\n","        if save and val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            path = os.path.join(drive_path, \"MyDrive\", \"pointnetmodel.yml\")\n","            print(\"best_val_acc:\", val_acc, \"saving model at\", path)\n","            torch.save(pointnet.state_dict(), path)"]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":810299,"status":"ok","timestamp":1655802682123,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"pCioSo6kyU5M","outputId":"3320b262-30af-4532-fa9a-e14415cff07f"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,     1] loss: 0.119\n","[1,     2] loss: 0.120\n","[1,     3] loss: 0.119\n","[1,     4] loss: 0.113\n","[1,     5] loss: 0.111\n","[1,     6] loss: 0.113\n","[1,     7] loss: 0.111\n","[1,     8] loss: 0.108\n","[1,     9] loss: 0.112\n","[1,    10] loss: 0.112\n","[1,    11] loss: 0.108\n","[1,    12] loss: 0.114\n","[1,    13] loss: 0.109\n","[1,    14] loss: 0.104\n","[1,    15] loss: 0.105\n","[1,    16] loss: 0.101\n","[1,    17] loss: 0.104\n","[1,    18] loss: 0.100\n","[1,    19] loss: 0.094\n","[1,    20] loss: 0.096\n","correct 506 / 400000\n","Valid accuracy: 0 %\n","best_val_acc: 0.1265 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[2,     1] loss: 0.092\n","[2,     2] loss: 0.094\n","[2,     3] loss: 0.090\n","[2,     4] loss: 0.091\n","[2,     5] loss: 0.089\n","[2,     6] loss: 0.086\n","[2,     7] loss: 0.087\n","[2,     8] loss: 0.091\n","[2,     9] loss: 0.086\n","[2,    10] loss: 0.089\n","[2,    11] loss: 0.080\n","[2,    12] loss: 0.081\n","[2,    13] loss: 0.085\n","[2,    14] loss: 0.082\n","[2,    15] loss: 0.081\n","[2,    16] loss: 0.084\n","[2,    17] loss: 0.086\n","[2,    18] loss: 0.077\n","[2,    19] loss: 0.080\n","[2,    20] loss: 0.075\n","correct 228609 / 400000\n","Valid accuracy: 57 %\n","best_val_acc: 57.15225 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[3,     1] loss: 0.076\n","[3,     2] loss: 0.077\n","[3,     3] loss: 0.073\n","[3,     4] loss: 0.073\n","[3,     5] loss: 0.076\n","[3,     6] loss: 0.073\n","[3,     7] loss: 0.072\n","[3,     8] loss: 0.071\n","[3,     9] loss: 0.071\n","[3,    10] loss: 0.072\n","[3,    11] loss: 0.075\n","[3,    12] loss: 0.075\n","[3,    13] loss: 0.066\n","[3,    14] loss: 0.070\n","[3,    15] loss: 0.065\n","[3,    16] loss: 0.077\n","[3,    17] loss: 0.070\n","[3,    18] loss: 0.064\n","[3,    19] loss: 0.066\n","[3,    20] loss: 0.068\n","correct 232546 / 400000\n","Valid accuracy: 58 %\n","best_val_acc: 58.1365 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[4,     1] loss: 0.071\n","[4,     2] loss: 0.069\n","[4,     3] loss: 0.063\n","[4,     4] loss: 0.062\n","[4,     5] loss: 0.062\n","[4,     6] loss: 0.059\n","[4,     7] loss: 0.060\n","[4,     8] loss: 0.061\n","[4,     9] loss: 0.065\n","[4,    10] loss: 0.069\n","[4,    11] loss: 0.068\n","[4,    12] loss: 0.063\n","[4,    13] loss: 0.058\n","[4,    14] loss: 0.061\n","[4,    15] loss: 0.067\n","[4,    16] loss: 0.071\n","[4,    17] loss: 0.062\n","[4,    18] loss: 0.064\n","[4,    19] loss: 0.071\n","[4,    20] loss: 0.074\n","correct 507 / 400000\n","Valid accuracy: 0 %\n","[5,     1] loss: 0.065\n","[5,     2] loss: 0.065\n","[5,     3] loss: 0.063\n","[5,     4] loss: 0.058\n","[5,     5] loss: 0.061\n","[5,     6] loss: 0.062\n","[5,     7] loss: 0.062\n","[5,     8] loss: 0.064\n","[5,     9] loss: 0.059\n","[5,    10] loss: 0.060\n","[5,    11] loss: 0.057\n","[5,    12] loss: 0.055\n","[5,    13] loss: 0.055\n","[5,    14] loss: 0.060\n","[5,    15] loss: 0.055\n","[5,    16] loss: 0.057\n","[5,    17] loss: 0.056\n","[5,    18] loss: 0.052\n","[5,    19] loss: 0.053\n","[5,    20] loss: 0.053\n","correct 282080 / 400000\n","Valid accuracy: 70 %\n","best_val_acc: 70.52 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[6,     1] loss: 0.054\n","[6,     2] loss: 0.050\n","[6,     3] loss: 0.053\n","[6,     4] loss: 0.048\n","[6,     5] loss: 0.046\n","[6,     6] loss: 0.044\n","[6,     7] loss: 0.045\n","[6,     8] loss: 0.045\n","[6,     9] loss: 0.046\n","[6,    10] loss: 0.046\n","[6,    11] loss: 0.049\n","[6,    12] loss: 0.049\n","[6,    13] loss: 0.044\n","[6,    14] loss: 0.049\n","[6,    15] loss: 0.040\n","[6,    16] loss: 0.047\n","[6,    17] loss: 0.042\n","[6,    18] loss: 0.046\n","[6,    19] loss: 0.048\n","[6,    20] loss: 0.042\n","correct 313344 / 400000\n","Valid accuracy: 78 %\n","best_val_acc: 78.336 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[7,     1] loss: 0.050\n","[7,     2] loss: 0.052\n","[7,     3] loss: 0.045\n","[7,     4] loss: 0.046\n","[7,     5] loss: 0.045\n","[7,     6] loss: 0.045\n","[7,     7] loss: 0.044\n","[7,     8] loss: 0.045\n","[7,     9] loss: 0.042\n","[7,    10] loss: 0.045\n","[7,    11] loss: 0.041\n","[7,    12] loss: 0.045\n","[7,    13] loss: 0.046\n","[7,    14] loss: 0.044\n","[7,    15] loss: 0.044\n","[7,    16] loss: 0.040\n","[7,    17] loss: 0.040\n","[7,    18] loss: 0.038\n","[7,    19] loss: 0.037\n","[7,    20] loss: 0.041\n","correct 282163 / 400000\n","Valid accuracy: 70 %\n","[8,     1] loss: 0.038\n","[8,     2] loss: 0.040\n","[8,     3] loss: 0.049\n","[8,     4] loss: 0.041\n","[8,     5] loss: 0.040\n","[8,     6] loss: 0.038\n","[8,     7] loss: 0.043\n","[8,     8] loss: 0.040\n","[8,     9] loss: 0.045\n","[8,    10] loss: 0.046\n","[8,    11] loss: 0.036\n","[8,    12] loss: 0.045\n","[8,    13] loss: 0.038\n","[8,    14] loss: 0.035\n","[8,    15] loss: 0.035\n","[8,    16] loss: 0.036\n","[8,    17] loss: 0.048\n","[8,    18] loss: 0.052\n","[8,    19] loss: 0.038\n","[8,    20] loss: 0.041\n","correct 351859 / 400000\n","Valid accuracy: 87 %\n","best_val_acc: 87.96475 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[9,     1] loss: 0.043\n","[9,     2] loss: 0.036\n","[9,     3] loss: 0.037\n","[9,     4] loss: 0.041\n","[9,     5] loss: 0.036\n","[9,     6] loss: 0.033\n","[9,     7] loss: 0.038\n","[9,     8] loss: 0.037\n","[9,     9] loss: 0.046\n","[9,    10] loss: 0.035\n","[9,    11] loss: 0.037\n","[9,    12] loss: 0.040\n","[9,    13] loss: 0.044\n","[9,    14] loss: 0.034\n","[9,    15] loss: 0.038\n","[9,    16] loss: 0.038\n","[9,    17] loss: 0.037\n","[9,    18] loss: 0.039\n","[9,    19] loss: 0.033\n","[9,    20] loss: 0.037\n","correct 368221 / 400000\n","Valid accuracy: 92 %\n","best_val_acc: 92.05525 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[10,     1] loss: 0.035\n","[10,     2] loss: 0.036\n","[10,     3] loss: 0.037\n","[10,     4] loss: 0.031\n","[10,     5] loss: 0.034\n","[10,     6] loss: 0.035\n","[10,     7] loss: 0.031\n","[10,     8] loss: 0.032\n","[10,     9] loss: 0.032\n","[10,    10] loss: 0.032\n","[10,    11] loss: 0.032\n","[10,    12] loss: 0.034\n","[10,    13] loss: 0.033\n","[10,    14] loss: 0.032\n","[10,    15] loss: 0.032\n","[10,    16] loss: 0.033\n","[10,    17] loss: 0.033\n","[10,    18] loss: 0.032\n","[10,    19] loss: 0.032\n","[10,    20] loss: 0.044\n","correct 269103 / 400000\n","Valid accuracy: 67 %\n","[11,     1] loss: 0.039\n","[11,     2] loss: 0.038\n","[11,     3] loss: 0.032\n","[11,     4] loss: 0.031\n","[11,     5] loss: 0.040\n","[11,     6] loss: 0.038\n","[11,     7] loss: 0.038\n","[11,     8] loss: 0.032\n","[11,     9] loss: 0.029\n","[11,    10] loss: 0.031\n","[11,    11] loss: 0.031\n","[11,    12] loss: 0.040\n","[11,    13] loss: 0.038\n","[11,    14] loss: 0.033\n","[11,    15] loss: 0.035\n","[11,    16] loss: 0.031\n","[11,    17] loss: 0.036\n","[11,    18] loss: 0.031\n","[11,    19] loss: 0.028\n","[11,    20] loss: 0.034\n","correct 379233 / 400000\n","Valid accuracy: 94 %\n","best_val_acc: 94.80825 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[12,     1] loss: 0.030\n","[12,     2] loss: 0.030\n","[12,     3] loss: 0.031\n","[12,     4] loss: 0.034\n","[12,     5] loss: 0.037\n","[12,     6] loss: 0.033\n","[12,     7] loss: 0.031\n","[12,     8] loss: 0.031\n","[12,     9] loss: 0.033\n","[12,    10] loss: 0.027\n","[12,    11] loss: 0.029\n","[12,    12] loss: 0.028\n","[12,    13] loss: 0.040\n","[12,    14] loss: 0.029\n","[12,    15] loss: 0.025\n","[12,    16] loss: 0.027\n","[12,    17] loss: 0.026\n","[12,    18] loss: 0.035\n","[12,    19] loss: 0.031\n","[12,    20] loss: 0.028\n","correct 357368 / 400000\n","Valid accuracy: 89 %\n","[13,     1] loss: 0.029\n","[13,     2] loss: 0.025\n","[13,     3] loss: 0.035\n","[13,     4] loss: 0.030\n","[13,     5] loss: 0.032\n","[13,     6] loss: 0.026\n","[13,     7] loss: 0.026\n","[13,     8] loss: 0.028\n","[13,     9] loss: 0.032\n","[13,    10] loss: 0.029\n","[13,    11] loss: 0.031\n","[13,    12] loss: 0.030\n","[13,    13] loss: 0.027\n","[13,    14] loss: 0.025\n","[13,    15] loss: 0.027\n","[13,    16] loss: 0.028\n","[13,    17] loss: 0.027\n","[13,    18] loss: 0.031\n","[13,    19] loss: 0.028\n","[13,    20] loss: 0.031\n","correct 372034 / 400000\n","Valid accuracy: 93 %\n","[14,     1] loss: 0.025\n","[14,     2] loss: 0.029\n","[14,     3] loss: 0.028\n","[14,     4] loss: 0.037\n","[14,     5] loss: 0.031\n","[14,     6] loss: 0.034\n","[14,     7] loss: 0.035\n","[14,     8] loss: 0.035\n","[14,     9] loss: 0.035\n","[14,    10] loss: 0.031\n","[14,    11] loss: 0.027\n","[14,    12] loss: 0.025\n","[14,    13] loss: 0.030\n","[14,    14] loss: 0.027\n","[14,    15] loss: 0.024\n","[14,    16] loss: 0.030\n","[14,    17] loss: 0.029\n","[14,    18] loss: 0.030\n","[14,    19] loss: 0.029\n","[14,    20] loss: 0.030\n","correct 344537 / 400000\n","Valid accuracy: 86 %\n","[15,     1] loss: 0.035\n","[15,     2] loss: 0.027\n","[15,     3] loss: 0.023\n","[15,     4] loss: 0.032\n","[15,     5] loss: 0.030\n","[15,     6] loss: 0.026\n","[15,     7] loss: 0.030\n","[15,     8] loss: 0.029\n","[15,     9] loss: 0.031\n","[15,    10] loss: 0.026\n","[15,    11] loss: 0.027\n","[15,    12] loss: 0.030\n","[15,    13] loss: 0.028\n","[15,    14] loss: 0.028\n","[15,    15] loss: 0.035\n","[15,    16] loss: 0.026\n","[15,    17] loss: 0.027\n","[15,    18] loss: 0.034\n","[15,    19] loss: 0.033\n","[15,    20] loss: 0.030\n","correct 352441 / 400000\n","Valid accuracy: 88 %\n"]}],"source":["train(pointnet, train_loader, val_loader, save=True)"]},{"cell_type":"markdown","metadata":{"id":"irrVJH1UJb92"},"source":["# Test\n","Let's compute the model test metrics\n","\n","> First we need to load the best model weights\n","\n","\n","\n"]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1655802682124,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"3o2IwxbfA9sc","outputId":"d0af833b-f77e-421c-a516-256ca3b4972d"},"outputs":[{"data":{"text/plain":["PointNetSeg(\n","  (pointnet): PointNet(\n","    (input_transform): TNet(\n","      (mlp1): MLP(\n","        (conv): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp2): MLP(\n","        (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp3): MLP(\n","        (conv): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn1): FC_BN(\n","        (lin): Linear(in_features=1024, out_features=512, bias=True)\n","        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn2): FC_BN(\n","        (lin): Linear(in_features=512, out_features=256, bias=True)\n","        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc3): Linear(in_features=256, out_features=9, bias=True)\n","    )\n","    (mlp1_1): MLP(\n","      (conv): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp1_2): MLP(\n","      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (feature_transform): TNet(\n","      (mlp1): MLP(\n","        (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp2): MLP(\n","        (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp3): MLP(\n","        (conv): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn1): FC_BN(\n","        (lin): Linear(in_features=1024, out_features=512, bias=True)\n","        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn2): FC_BN(\n","        (lin): Linear(in_features=512, out_features=256, bias=True)\n","        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc3): Linear(in_features=256, out_features=4096, bias=True)\n","    )\n","    (mlp2_1): MLP(\n","      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_2): MLP(\n","      (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_3): MLP(\n","      (conv): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (mlp1_1): MLP(\n","    (conv): Conv1d(1088, 512, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp1_2): MLP(\n","    (conv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp1_3): MLP(\n","    (conv): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp2_1): MLP(\n","    (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp2_2): MLP(\n","    (conv): Conv1d(128, 3, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (logsoftmax): LogSoftmax(dim=1)\n",")"]},"execution_count":30,"metadata":{},"output_type":"execute_result"}],"source":["# create a new instantiation of PointNetSeg model\n","pointnet = PointNetSeg()\n","\n","# load pyTorch model weights\n","model_path = os.path.join(drive_path, \"MyDrive\", \"pointnetmodel.yml\")\n","pointnet.load_state_dict(torch.load(model_path))\n","\n","# move the model to cuda\n","pointnet.to(device)"]},{"cell_type":"code","execution_count":31,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655802682125,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"eql3zbw3HJU4"},"outputs":[],"source":["def compute_stats(true_labels, pred_labels):\n","  unk     = np.count_nonzero(true_labels == 0)\n","  trav    = np.count_nonzero(true_labels == 1)\n","  nontrav = np.count_nonzero(true_labels == 2)\n","\n","  total_predictions = labels.shape[1]*labels.shape[0]\n","  correct = (true_labels == pred_labels).sum().item()\n","\n","  return correct, total_predictions"]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24066,"status":"ok","timestamp":1655802706183,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"u9W8J2SIDoO2","outputId":"c7cd133f-cadf-4336-e390-5634fe915e84"},"outputs":[{"name":"stderr","output_type":"stream","text":["30it [00:23,  1.26it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Test accuracy: 81.52766666666666 %\n","total time: 23.85502028465271  [s]\n","avg time  : 0.795167342821757  [s]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["pointnet.eval()\n","total_correct_predictions = total_predictions = 0\n","\n","start = time.time()\n","\n","for i, data in tqdm(enumerate(test_loader, 0)):\n","  inputs, labels = data\n","  inputs = inputs.to(device).float()\n","  labels = labels.to(device)\n","  outputs, __, __ = pointnet(inputs.transpose(1,2))  \n","  _, predicted = torch.max(outputs.data, 1)\n","\n","  # visualize results\n","  remapped_pred = remap_to_bgr(predicted[0].cpu().numpy(), remap_color_scheme)\n","  np_pointcloud = inputs[0].cpu().numpy()\n","  # visualize3DPointCloud(np_pointcloud, remapped_pred)\n","  \n","  # compute statistics\n","  ground_truth_labels = labels.cpu()\n","  predicted_labels    = predicted.cpu()\n","  correct, total = compute_stats(ground_truth_labels, predicted_labels)\n","\n","  total_correct_predictions += correct\n","  total_predictions         += total\n","\n","end = time.time()\n","\n","# nice layout after tqdm\n","print()\n","print()\n","\n","test_acc    = 100. * total_correct_predictions / total_predictions\n","tot_latency = end-start\n","avg_latency = tot_latency / len(test_loader.dataset)\n","\n","print('Test accuracy:', test_acc, \"%\")\n","print('total time:',    tot_latency, \" [s]\")\n","print('avg time  :',    avg_latency, \" [s]\")\n"]},{"cell_type":"markdown","metadata":{"id":"xAE3Jh5Rz6pi"},"source":["> **Note**: you need to write *Test accuracy* and *avg time* in your laboratory report\n","\n","\n","\n","If you want to experiment with other Network structure, feel free to modify the original PointNetSeg implementation.\n","\n","For example, you may change layers input and output size parameters, or directly adding layers. You need to be very careful in doing such operations.\n","\n","These experiments are not mandatory but mentioning them in the report will be appreciated. **Note that the report and the source code that you deliver must contain also the original version required by this laboratory.**\n","\n","**So every modification you may do need to follow this cell: create new cells if needed.**"]},{"cell_type":"markdown","metadata":{"id":"LJw0YlzW3U9k"},"source":["# Experiment 1\n","\n","The PointNetSeg Module is changed.<br>"]},{"cell_type":"markdown","metadata":{"id":"vvu9Zxgg3kHZ"},"source":["## Network redefinition"]},{"cell_type":"markdown","metadata":{"id":"inYeyCzI5TIM"},"source":["### PointNet Module"]},{"cell_type":"code","execution_count":33,"metadata":{"cellView":"form","executionInfo":{"elapsed":6,"status":"ok","timestamp":1655802706184,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"Umr3sdkx5TIT"},"outputs":[],"source":["class PointNet(nn.Module):\n","   def __init__(self):\n","        super().__init__()\n","        self.input_transform   = TNet(k=3)\n","        \n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        self.mlp1_1 = MLP(3, 64)\n","        self.mlp1_2 = MLP(64, 64)\n","\n","        self.feature_transform = TNet(k=64)\n","        \n","        self.mlp2_1 = MLP(64, 64)\n","        self.mlp2_2 = MLP(64, 128)\n","        self.mlp2_3 = MLP(128, 1024)\n","\n","        ###########################################################\n","\n","\n","   def forward(self, input):\n","        n_pts = input.size()[2]\n","        matrix3x3 = self.input_transform(input)\n","        input_transform_output = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n","\n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        xb = self.mlp1_1(input_transform_output)\n","        xb = self.mlp1_2(xb)\n","\n","        #FEATURE TRANSFORM PART\n","        #computation feature transform by using T-Net\n","        matrix64x64 = self.feature_transform(xb)\n","        #matrix multiplication among features and feature transform matrix\n","        feature_transform_output = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n","\n","        xb = self.mlp2_1(feature_transform_output)\n","        xb = self.mlp2_2(xb)\n","        xb = self.mlp2_3(xb) \n","\n","        pool = nn.MaxPool1d(xb.size(-1))(xb)\n","        global_feature = nn.Flatten(1)(pool)\n","        \n","        ###########################################################\n","\n","        global_feature_repeated = nn.Flatten(1)(global_feature).repeat(n_pts,1,1).transpose(0,2).transpose(0,1)\n","\n","        return [feature_transform_output, global_feature_repeated], matrix3x3, matrix64x64"]},{"cell_type":"markdown","metadata":{"id":"SRWUsihE3tzc"},"source":["### PointNetSeg Module\n","\n","With respect with the original implementation it is taken off the 128 neuron layer among the point features matrix and the output.<br>"]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1655802706184,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"V61hKt8o3x6L"},"outputs":[],"source":["class PointNetSeg(nn.Module):\n","    def __init__(self, classes = 3):\n","        super().__init__()\n","        self.pointnet = PointNet()\n","\n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","        \n","        self.mlp1_1 = MLP(1088,512)\n","        self.mlp1_2 = MLP(512,256)\n","        self.mlp1_3 = MLP(256,128)\n","\n","        self.mlp2 = MLP(128,classes)\n","\n","        ###########################################################\n","\n","        self.logsoftmax = nn.LogSoftmax(dim=1)\n","        \n","\n","    def forward(self, input):\n","        inputs, matrix3x3, matrix64x64 = self.pointnet(input)\n","        stack = torch.cat(inputs,1)\n","        \n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        xb = self.mlp1_1(stack)\n","        xb = self.mlp1_2(xb)\n","        point_features = self.mlp1_3(xb)\n","           \n","        output = self.mlp2(point_features)\n","\n","        ###########################################################\n","\n","        return self.logsoftmax(output), matrix3x3, matrix64x64"]},{"cell_type":"markdown","metadata":{"id":"85v2RUhV4Z9s"},"source":["## Training loop"]},{"cell_type":"code","execution_count":35,"metadata":{"cellView":"form","executionInfo":{"elapsed":5,"status":"ok","timestamp":1655802706184,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"JkMoG2C25ARr"},"outputs":[],"source":["pointnet = PointNetSeg()\n","pointnet.to(device);"]},{"cell_type":"code","execution_count":36,"metadata":{"cellView":"form","executionInfo":{"elapsed":6,"status":"ok","timestamp":1655802706185,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"Dem349-S5ARs"},"outputs":[],"source":["optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.005)"]},{"cell_type":"code","execution_count":37,"metadata":{"cellView":"form","executionInfo":{"elapsed":5,"status":"ok","timestamp":1655802706185,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"s9iGmsqu5ARs"},"outputs":[],"source":["def train(model, train_loader, val_loader=None,  epochs=15, save=True):\n","    best_val_acc = -1.0\n","    for epoch in range(epochs): \n","        pointnet.train()\n","        running_loss = 0.0\n","\n","        for i, data in enumerate(train_loader, 0):\n","            inputs, labels = data\n","            inputs = inputs.to(device).float()\n","            labels = labels.to(device)\n","            optimizer.zero_grad()\n","            outputs, m3x3, m64x64 = pointnet(inputs.transpose(1,2))\n","            loss = pointNetLoss(outputs, labels, m3x3, m64x64)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            if i % 10 == 9 or True:    # print every 10 mini-batches\n","                    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n","                    running_loss = 0.0\n","\n","        pointnet.eval()\n","        correct = total = 0\n","\n","        # validation\n","        with torch.no_grad():\n","            for data in val_loader:\n","                inputs, labels = data\n","                inputs = inputs.to(device).float()\n","                labels = labels.to(device)\n","                outputs, __, __ = pointnet(inputs.transpose(1,2))\n","                _, predicted = torch.max(outputs.data, 1)\n","                \n","                total   += labels.size(0) * labels.size(1)\n","                correct += (predicted == labels).sum().item()\n","\n","        print(\"correct\", correct, \"/\", total)\n","        val_acc = 100.0 * correct / total\n","        print('Valid accuracy: %d %%' % val_acc)\n","\n","        # save the model\n","        if save and val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            path = os.path.join(drive_path, \"MyDrive\", \"pointnetmodel.yml\")\n","            print(\"best_val_acc:\", val_acc, \"saving model at\", path)\n","            torch.save(pointnet.state_dict(), path)"]},{"cell_type":"code","execution_count":38,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":740778,"status":"ok","timestamp":1655803446958,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"eLCN0KE85ARs","outputId":"a86e6d3a-d926-44c9-87a4-a84824d7955d"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,     1] loss: 0.122\n","[1,     2] loss: 0.118\n","[1,     3] loss: 0.111\n","[1,     4] loss: 0.100\n","[1,     5] loss: 0.100\n","[1,     6] loss: 0.099\n","[1,     7] loss: 0.098\n","[1,     8] loss: 0.099\n","[1,     9] loss: 0.098\n","[1,    10] loss: 0.097\n","[1,    11] loss: 0.097\n","[1,    12] loss: 0.094\n","[1,    13] loss: 0.094\n","[1,    14] loss: 0.090\n","[1,    15] loss: 0.091\n","[1,    16] loss: 0.090\n","[1,    17] loss: 0.094\n","[1,    18] loss: 0.086\n","[1,    19] loss: 0.085\n","[1,    20] loss: 0.087\n","correct 150365 / 400000\n","Valid accuracy: 37 %\n","best_val_acc: 37.59125 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[2,     1] loss: 0.085\n","[2,     2] loss: 0.085\n","[2,     3] loss: 0.084\n","[2,     4] loss: 0.083\n","[2,     5] loss: 0.081\n","[2,     6] loss: 0.080\n","[2,     7] loss: 0.079\n","[2,     8] loss: 0.077\n","[2,     9] loss: 0.076\n","[2,    10] loss: 0.081\n","[2,    11] loss: 0.080\n","[2,    12] loss: 0.074\n","[2,    13] loss: 0.074\n","[2,    14] loss: 0.071\n","[2,    15] loss: 0.082\n","[2,    16] loss: 0.076\n","[2,    17] loss: 0.079\n","[2,    18] loss: 0.073\n","[2,    19] loss: 0.069\n","[2,    20] loss: 0.074\n","correct 86495 / 400000\n","Valid accuracy: 21 %\n","[3,     1] loss: 0.066\n","[3,     2] loss: 0.071\n","[3,     3] loss: 0.067\n","[3,     4] loss: 0.068\n","[3,     5] loss: 0.070\n","[3,     6] loss: 0.065\n","[3,     7] loss: 0.065\n","[3,     8] loss: 0.068\n","[3,     9] loss: 0.066\n","[3,    10] loss: 0.064\n","[3,    11] loss: 0.065\n","[3,    12] loss: 0.065\n","[3,    13] loss: 0.069\n","[3,    14] loss: 0.061\n","[3,    15] loss: 0.063\n","[3,    16] loss: 0.062\n","[3,    17] loss: 0.064\n","[3,    18] loss: 0.063\n","[3,    19] loss: 0.059\n","[3,    20] loss: 0.056\n","correct 19069 / 400000\n","Valid accuracy: 4 %\n","[4,     1] loss: 0.062\n","[4,     2] loss: 0.056\n","[4,     3] loss: 0.056\n","[4,     4] loss: 0.060\n","[4,     5] loss: 0.058\n","[4,     6] loss: 0.055\n","[4,     7] loss: 0.058\n","[4,     8] loss: 0.053\n","[4,     9] loss: 0.052\n","[4,    10] loss: 0.053\n","[4,    11] loss: 0.053\n","[4,    12] loss: 0.051\n","[4,    13] loss: 0.050\n","[4,    14] loss: 0.051\n","[4,    15] loss: 0.047\n","[4,    16] loss: 0.056\n","[4,    17] loss: 0.049\n","[4,    18] loss: 0.050\n","[4,    19] loss: 0.053\n","[4,    20] loss: 0.050\n","correct 288957 / 400000\n","Valid accuracy: 72 %\n","best_val_acc: 72.23925 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[5,     1] loss: 0.048\n","[5,     2] loss: 0.049\n","[5,     3] loss: 0.048\n","[5,     4] loss: 0.051\n","[5,     5] loss: 0.053\n","[5,     6] loss: 0.052\n","[5,     7] loss: 0.049\n","[5,     8] loss: 0.062\n","[5,     9] loss: 0.047\n","[5,    10] loss: 0.056\n","[5,    11] loss: 0.048\n","[5,    12] loss: 0.049\n","[5,    13] loss: 0.049\n","[5,    14] loss: 0.049\n","[5,    15] loss: 0.048\n","[5,    16] loss: 0.051\n","[5,    17] loss: 0.045\n","[5,    18] loss: 0.046\n","[5,    19] loss: 0.049\n","[5,    20] loss: 0.045\n","correct 233781 / 400000\n","Valid accuracy: 58 %\n","[6,     1] loss: 0.047\n","[6,     2] loss: 0.045\n","[6,     3] loss: 0.043\n","[6,     4] loss: 0.041\n","[6,     5] loss: 0.042\n","[6,     6] loss: 0.044\n","[6,     7] loss: 0.038\n","[6,     8] loss: 0.040\n","[6,     9] loss: 0.039\n","[6,    10] loss: 0.051\n","[6,    11] loss: 0.043\n","[6,    12] loss: 0.040\n","[6,    13] loss: 0.043\n","[6,    14] loss: 0.057\n","[6,    15] loss: 0.041\n","[6,    16] loss: 0.043\n","[6,    17] loss: 0.045\n","[6,    18] loss: 0.041\n","[6,    19] loss: 0.042\n","[6,    20] loss: 0.041\n","correct 361107 / 400000\n","Valid accuracy: 90 %\n","best_val_acc: 90.27675 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[7,     1] loss: 0.044\n","[7,     2] loss: 0.043\n","[7,     3] loss: 0.040\n","[7,     4] loss: 0.044\n","[7,     5] loss: 0.044\n","[7,     6] loss: 0.042\n","[7,     7] loss: 0.040\n","[7,     8] loss: 0.042\n","[7,     9] loss: 0.043\n","[7,    10] loss: 0.039\n","[7,    11] loss: 0.043\n","[7,    12] loss: 0.041\n","[7,    13] loss: 0.038\n","[7,    14] loss: 0.035\n","[7,    15] loss: 0.038\n","[7,    16] loss: 0.037\n","[7,    17] loss: 0.038\n","[7,    18] loss: 0.042\n","[7,    19] loss: 0.040\n","[7,    20] loss: 0.039\n","correct 311312 / 400000\n","Valid accuracy: 77 %\n","[8,     1] loss: 0.037\n","[8,     2] loss: 0.037\n","[8,     3] loss: 0.035\n","[8,     4] loss: 0.041\n","[8,     5] loss: 0.042\n","[8,     6] loss: 0.038\n","[8,     7] loss: 0.035\n","[8,     8] loss: 0.033\n","[8,     9] loss: 0.036\n","[8,    10] loss: 0.034\n","[8,    11] loss: 0.039\n","[8,    12] loss: 0.042\n","[8,    13] loss: 0.040\n","[8,    14] loss: 0.040\n","[8,    15] loss: 0.035\n","[8,    16] loss: 0.039\n","[8,    17] loss: 0.034\n","[8,    18] loss: 0.035\n","[8,    19] loss: 0.038\n","[8,    20] loss: 0.035\n","correct 365729 / 400000\n","Valid accuracy: 91 %\n","best_val_acc: 91.43225 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[9,     1] loss: 0.037\n","[9,     2] loss: 0.036\n","[9,     3] loss: 0.034\n","[9,     4] loss: 0.033\n","[9,     5] loss: 0.034\n","[9,     6] loss: 0.039\n","[9,     7] loss: 0.038\n","[9,     8] loss: 0.030\n","[9,     9] loss: 0.038\n","[9,    10] loss: 0.035\n","[9,    11] loss: 0.033\n","[9,    12] loss: 0.036\n","[9,    13] loss: 0.032\n","[9,    14] loss: 0.038\n","[9,    15] loss: 0.032\n","[9,    16] loss: 0.034\n","[9,    17] loss: 0.032\n","[9,    18] loss: 0.033\n","[9,    19] loss: 0.034\n","[9,    20] loss: 0.034\n","correct 349278 / 400000\n","Valid accuracy: 87 %\n","[10,     1] loss: 0.033\n","[10,     2] loss: 0.041\n","[10,     3] loss: 0.032\n","[10,     4] loss: 0.034\n","[10,     5] loss: 0.041\n","[10,     6] loss: 0.033\n","[10,     7] loss: 0.033\n","[10,     8] loss: 0.043\n","[10,     9] loss: 0.035\n","[10,    10] loss: 0.047\n","[10,    11] loss: 0.034\n","[10,    12] loss: 0.037\n","[10,    13] loss: 0.037\n","[10,    14] loss: 0.039\n","[10,    15] loss: 0.042\n","[10,    16] loss: 0.042\n","[10,    17] loss: 0.044\n","[10,    18] loss: 0.032\n","[10,    19] loss: 0.038\n","[10,    20] loss: 0.037\n","correct 367243 / 400000\n","Valid accuracy: 91 %\n","best_val_acc: 91.81075 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[11,     1] loss: 0.037\n","[11,     2] loss: 0.033\n","[11,     3] loss: 0.033\n","[11,     4] loss: 0.034\n","[11,     5] loss: 0.038\n","[11,     6] loss: 0.033\n","[11,     7] loss: 0.036\n","[11,     8] loss: 0.038\n","[11,     9] loss: 0.033\n","[11,    10] loss: 0.033\n","[11,    11] loss: 0.035\n","[11,    12] loss: 0.045\n","[11,    13] loss: 0.031\n","[11,    14] loss: 0.030\n","[11,    15] loss: 0.037\n","[11,    16] loss: 0.035\n","[11,    17] loss: 0.037\n","[11,    18] loss: 0.038\n","[11,    19] loss: 0.030\n","[11,    20] loss: 0.036\n","correct 285895 / 400000\n","Valid accuracy: 71 %\n","[12,     1] loss: 0.036\n","[12,     2] loss: 0.044\n","[12,     3] loss: 0.050\n","[12,     4] loss: 0.038\n","[12,     5] loss: 0.041\n","[12,     6] loss: 0.039\n","[12,     7] loss: 0.041\n","[12,     8] loss: 0.039\n","[12,     9] loss: 0.038\n","[12,    10] loss: 0.038\n","[12,    11] loss: 0.037\n","[12,    12] loss: 0.038\n","[12,    13] loss: 0.035\n","[12,    14] loss: 0.038\n","[12,    15] loss: 0.037\n","[12,    16] loss: 0.033\n","[12,    17] loss: 0.041\n","[12,    18] loss: 0.035\n","[12,    19] loss: 0.031\n","[12,    20] loss: 0.033\n","correct 356352 / 400000\n","Valid accuracy: 89 %\n","[13,     1] loss: 0.030\n","[13,     2] loss: 0.032\n","[13,     3] loss: 0.042\n","[13,     4] loss: 0.037\n","[13,     5] loss: 0.034\n","[13,     6] loss: 0.031\n","[13,     7] loss: 0.032\n","[13,     8] loss: 0.033\n","[13,     9] loss: 0.029\n","[13,    10] loss: 0.031\n","[13,    11] loss: 0.030\n","[13,    12] loss: 0.029\n","[13,    13] loss: 0.032\n","[13,    14] loss: 0.028\n","[13,    15] loss: 0.029\n","[13,    16] loss: 0.032\n","[13,    17] loss: 0.032\n","[13,    18] loss: 0.035\n","[13,    19] loss: 0.030\n","[13,    20] loss: 0.034\n","correct 334482 / 400000\n","Valid accuracy: 83 %\n","[14,     1] loss: 0.027\n","[14,     2] loss: 0.027\n","[14,     3] loss: 0.033\n","[14,     4] loss: 0.031\n","[14,     5] loss: 0.029\n","[14,     6] loss: 0.028\n","[14,     7] loss: 0.029\n","[14,     8] loss: 0.034\n","[14,     9] loss: 0.026\n","[14,    10] loss: 0.029\n","[14,    11] loss: 0.034\n","[14,    12] loss: 0.036\n","[14,    13] loss: 0.034\n","[14,    14] loss: 0.033\n","[14,    15] loss: 0.030\n","[14,    16] loss: 0.030\n","[14,    17] loss: 0.033\n","[14,    18] loss: 0.034\n","[14,    19] loss: 0.033\n","[14,    20] loss: 0.030\n","correct 301902 / 400000\n","Valid accuracy: 75 %\n","[15,     1] loss: 0.032\n","[15,     2] loss: 0.030\n","[15,     3] loss: 0.030\n","[15,     4] loss: 0.030\n","[15,     5] loss: 0.027\n","[15,     6] loss: 0.032\n","[15,     7] loss: 0.032\n","[15,     8] loss: 0.028\n","[15,     9] loss: 0.029\n","[15,    10] loss: 0.031\n","[15,    11] loss: 0.033\n","[15,    12] loss: 0.033\n","[15,    13] loss: 0.028\n","[15,    14] loss: 0.028\n","[15,    15] loss: 0.029\n","[15,    16] loss: 0.032\n","[15,    17] loss: 0.027\n","[15,    18] loss: 0.031\n","[15,    19] loss: 0.030\n","[15,    20] loss: 0.029\n","correct 341644 / 400000\n","Valid accuracy: 85 %\n"]}],"source":["train(pointnet, train_loader, val_loader, save=True)"]},{"cell_type":"markdown","metadata":{"id":"RFCEWxdh5ARs"},"source":["## Test\n","\n","\n","\n"]},{"cell_type":"code","execution_count":39,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1655803446958,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"J2QF6UQN5ARt","outputId":"faa430d8-f799-483b-d6a5-2b16462e3b47"},"outputs":[{"data":{"text/plain":["PointNetSeg(\n","  (pointnet): PointNet(\n","    (input_transform): TNet(\n","      (mlp1): MLP(\n","        (conv): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp2): MLP(\n","        (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp3): MLP(\n","        (conv): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn1): FC_BN(\n","        (lin): Linear(in_features=1024, out_features=512, bias=True)\n","        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn2): FC_BN(\n","        (lin): Linear(in_features=512, out_features=256, bias=True)\n","        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc3): Linear(in_features=256, out_features=9, bias=True)\n","    )\n","    (mlp1_1): MLP(\n","      (conv): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp1_2): MLP(\n","      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (feature_transform): TNet(\n","      (mlp1): MLP(\n","        (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp2): MLP(\n","        (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp3): MLP(\n","        (conv): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn1): FC_BN(\n","        (lin): Linear(in_features=1024, out_features=512, bias=True)\n","        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn2): FC_BN(\n","        (lin): Linear(in_features=512, out_features=256, bias=True)\n","        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc3): Linear(in_features=256, out_features=4096, bias=True)\n","    )\n","    (mlp2_1): MLP(\n","      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_2): MLP(\n","      (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_3): MLP(\n","      (conv): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (mlp1_1): MLP(\n","    (conv): Conv1d(1088, 512, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp1_2): MLP(\n","    (conv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp1_3): MLP(\n","    (conv): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp2): MLP(\n","    (conv): Conv1d(128, 3, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (logsoftmax): LogSoftmax(dim=1)\n",")"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["# create a new instantiation of PointNetSeg model\n","pointnet = PointNetSeg()\n","\n","# load pyTorch model weights\n","model_path = os.path.join(drive_path, \"MyDrive\", \"pointnetmodel.yml\")\n","pointnet.load_state_dict(torch.load(model_path))\n","\n","# move the model to cuda\n","pointnet.to(device)"]},{"cell_type":"code","execution_count":40,"metadata":{"cellView":"form","executionInfo":{"elapsed":7,"status":"ok","timestamp":1655803446958,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"0JyOpsgA5ARt"},"outputs":[],"source":["def compute_stats(true_labels, pred_labels):\n","  unk     = np.count_nonzero(true_labels == 0)\n","  trav    = np.count_nonzero(true_labels == 1)\n","  nontrav = np.count_nonzero(true_labels == 2)\n","\n","  total_predictions = labels.shape[1]*labels.shape[0]\n","  correct = (true_labels == pred_labels).sum().item()\n","\n","  return correct, total_predictions"]},{"cell_type":"code","execution_count":41,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13084,"status":"ok","timestamp":1655803460035,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"E1HNhAZk5ARt","outputId":"e57ae6c1-49f1-421c-be29-348e94435f45"},"outputs":[{"name":"stderr","output_type":"stream","text":["30it [00:12,  2.31it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Test accuracy: 89.396 %\n","total time: 12.973624229431152  [s]\n","avg time  : 0.4324541409810384  [s]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["pointnet.eval()\n","total_correct_predictions = total_predictions = 0\n","\n","start = time.time()\n","\n","for i, data in tqdm(enumerate(test_loader, 0)):\n","  inputs, labels = data\n","  inputs = inputs.to(device).float()\n","  labels = labels.to(device)\n","  outputs, __, __ = pointnet(inputs.transpose(1,2))  \n","  _, predicted = torch.max(outputs.data, 1)\n","\n","  # visualize results\n","  remapped_pred = remap_to_bgr(predicted[0].cpu().numpy(), remap_color_scheme)\n","  np_pointcloud = inputs[0].cpu().numpy()\n","  # visualize3DPointCloud(np_pointcloud, remapped_pred)\n","  \n","  # compute statistics\n","  ground_truth_labels = labels.cpu()\n","  predicted_labels    = predicted.cpu()\n","  correct, total = compute_stats(ground_truth_labels, predicted_labels)\n","\n","  total_correct_predictions += correct\n","  total_predictions         += total\n","\n","end = time.time()\n","\n","# nice layout after tqdm\n","print()\n","print()\n","\n","test_acc    = 100. * total_correct_predictions / total_predictions\n","tot_latency = end-start\n","avg_latency = tot_latency / len(test_loader.dataset)\n","\n","print('Test accuracy:', test_acc, \"%\")\n","print('total time:',    tot_latency, \" [s]\")\n","print('avg time  :',    avg_latency, \" [s]\")\n"]},{"cell_type":"markdown","metadata":{"id":"tPV49NH_5ARt"},"source":["# Experiment 2\n","\n","The PointNet Module is changed.<br>"]},{"cell_type":"markdown","metadata":{"id":"0FYKmZo86PMU"},"source":["## Network redefinition"]},{"cell_type":"markdown","metadata":{"id":"th6wopgp6PMV"},"source":["### PointNet \n","\n","After the feature transformation it is added 2 layers to the neural network."]},{"cell_type":"code","execution_count":42,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1655803460035,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"982VkW4Q6PMV"},"outputs":[],"source":["class PointNet(nn.Module):\n","   def __init__(self):\n","        super().__init__()\n","        self.input_transform   = TNet(k=3)\n","        \n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        self.mlp1_1 = MLP(3, 64)\n","        self.mlp1_2 = MLP(64, 64)\n","\n","        self.feature_transform = TNet(k=64)\n","        \n","        self.mlp2_1 = MLP(64, 64)\n","        self.mlp2_2 = MLP(64, 128)\n","        self.mlp2_3 = MLP(128, 256)\n","        self.mlp2_4 = MLP(256, 512)\n","        self.mlp2_5 = MLP(512, 1024)\n","\n","        ###########################################################\n","\n","\n","   def forward(self, input):\n","        n_pts = input.size()[2]\n","        matrix3x3 = self.input_transform(input)\n","        input_transform_output = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n","\n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        xb = self.mlp1_1(input_transform_output)\n","        xb = self.mlp1_2(xb)\n","\n","        #FEATURE TRANSFORM PART\n","        #computation feature transform by using T-Net\n","        matrix64x64 = self.feature_transform(xb)\n","        #matrix multiplication among features and feature transform matrix\n","        feature_transform_output = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n","\n","        xb = self.mlp2_1(feature_transform_output)\n","        xb = self.mlp2_2(xb)\n","        xb = self.mlp2_3(xb)\n","        xb = self.mlp2_4(xb)\n","        xb = self.mlp2_5(xb) \n","\n","        pool = nn.MaxPool1d(xb.size(-1))(xb)\n","        global_feature = nn.Flatten(1)(pool)\n","        \n","        ###########################################################\n","\n","        global_feature_repeated = nn.Flatten(1)(global_feature).repeat(n_pts,1,1).transpose(0,2).transpose(0,1)\n","\n","        return [feature_transform_output, global_feature_repeated], matrix3x3, matrix64x64"]},{"cell_type":"markdown","metadata":{"id":"ieS2KOFh6PMV"},"source":["### PointNetSeg Module"]},{"cell_type":"code","execution_count":43,"metadata":{"cellView":"form","executionInfo":{"elapsed":13,"status":"ok","timestamp":1655803460036,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"wQkbJUw66PMW"},"outputs":[],"source":["class PointNetSeg(nn.Module):\n","    def __init__(self, classes = 3):\n","        super().__init__()\n","        self.pointnet = PointNet()\n","\n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","        \n","        self.mlp1_1 = MLP(1088,512)\n","        self.mlp1_2 = MLP(512,256)\n","        self.mlp1_3 = MLP(256,128)\n","\n","        self.mlp2_1 = MLP(128,128)\n","        self.mlp2_2 = MLP(128,classes)\n","\n","        ###########################################################\n","\n","        self.logsoftmax = nn.LogSoftmax(dim=1)\n","        \n","\n","    def forward(self, input):\n","        inputs, matrix3x3, matrix64x64 = self.pointnet(input)\n","        stack = torch.cat(inputs,1)\n","        \n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        xb = self.mlp1_1(stack)\n","        xb = self.mlp1_2(xb)\n","        point_features = self.mlp1_3(xb)\n","        \n","        xb = self.mlp2_1(point_features)\n","        output = self.mlp2_2(xb)\n","\n","        ###########################################################\n","\n","        return self.logsoftmax(output), matrix3x3, matrix64x64"]},{"cell_type":"markdown","metadata":{"id":"md6-aTyH6PMW"},"source":["## Training loop"]},{"cell_type":"code","execution_count":44,"metadata":{"cellView":"form","executionInfo":{"elapsed":12,"status":"ok","timestamp":1655803460036,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"Cm6GjoWY6PMX"},"outputs":[],"source":["pointnet = PointNetSeg()\n","pointnet.to(device);"]},{"cell_type":"code","execution_count":45,"metadata":{"cellView":"form","executionInfo":{"elapsed":12,"status":"ok","timestamp":1655803460036,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"tBGFP38Y6PMX"},"outputs":[],"source":["optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.005)"]},{"cell_type":"code","execution_count":46,"metadata":{"cellView":"form","executionInfo":{"elapsed":13,"status":"ok","timestamp":1655803460037,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"RKdtTDKU6PMX"},"outputs":[],"source":["def train(model, train_loader, val_loader=None,  epochs=15, save=True):\n","    best_val_acc = -1.0\n","    for epoch in range(epochs): \n","        pointnet.train()\n","        running_loss = 0.0\n","\n","        for i, data in enumerate(train_loader, 0):\n","            inputs, labels = data\n","            inputs = inputs.to(device).float()\n","            labels = labels.to(device)\n","            optimizer.zero_grad()\n","            outputs, m3x3, m64x64 = pointnet(inputs.transpose(1,2))\n","            loss = pointNetLoss(outputs, labels, m3x3, m64x64)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            if i % 10 == 9 or True:    # print every 10 mini-batches\n","                    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n","                    running_loss = 0.0\n","\n","        pointnet.eval()\n","        correct = total = 0\n","\n","        # validation\n","        with torch.no_grad():\n","            for data in val_loader:\n","                inputs, labels = data\n","                inputs = inputs.to(device).float()\n","                labels = labels.to(device)\n","                outputs, __, __ = pointnet(inputs.transpose(1,2))\n","                _, predicted = torch.max(outputs.data, 1)\n","                \n","                total   += labels.size(0) * labels.size(1)\n","                correct += (predicted == labels).sum().item()\n","\n","        print(\"correct\", correct, \"/\", total)\n","        val_acc = 100.0 * correct / total\n","        print('Valid accuracy: %d %%' % val_acc)\n","\n","        # save the model\n","        if save and val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            path = os.path.join(drive_path, \"MyDrive\", \"pointnetmodel.yml\")\n","            print(\"best_val_acc:\", val_acc, \"saving model at\", path)\n","            torch.save(pointnet.state_dict(), path)"]},{"cell_type":"code","execution_count":47,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":801305,"status":"ok","timestamp":1655804261330,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"yvxnpQTl6PMY","outputId":"36687545-213d-4366-8259-ef3750b63913"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,     1] loss: 0.124\n","[1,     2] loss: 0.118\n","[1,     3] loss: 0.115\n","[1,     4] loss: 0.116\n","[1,     5] loss: 0.116\n","[1,     6] loss: 0.110\n","[1,     7] loss: 0.107\n","[1,     8] loss: 0.110\n","[1,     9] loss: 0.105\n","[1,    10] loss: 0.102\n","[1,    11] loss: 0.100\n","[1,    12] loss: 0.100\n","[1,    13] loss: 0.098\n","[1,    14] loss: 0.092\n","[1,    15] loss: 0.097\n","[1,    16] loss: 0.099\n","[1,    17] loss: 0.094\n","[1,    18] loss: 0.091\n","[1,    19] loss: 0.091\n","[1,    20] loss: 0.090\n","correct 5658 / 400000\n","Valid accuracy: 1 %\n","best_val_acc: 1.4145 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[2,     1] loss: 0.090\n","[2,     2] loss: 0.097\n","[2,     3] loss: 0.087\n","[2,     4] loss: 0.088\n","[2,     5] loss: 0.087\n","[2,     6] loss: 0.097\n","[2,     7] loss: 0.086\n","[2,     8] loss: 0.089\n","[2,     9] loss: 0.087\n","[2,    10] loss: 0.083\n","[2,    11] loss: 0.081\n","[2,    12] loss: 0.082\n","[2,    13] loss: 0.079\n","[2,    14] loss: 0.076\n","[2,    15] loss: 0.077\n","[2,    16] loss: 0.077\n","[2,    17] loss: 0.074\n","[2,    18] loss: 0.077\n","[2,    19] loss: 0.074\n","[2,    20] loss: 0.078\n","correct 232644 / 400000\n","Valid accuracy: 58 %\n","best_val_acc: 58.161 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[3,     1] loss: 0.077\n","[3,     2] loss: 0.071\n","[3,     3] loss: 0.069\n","[3,     4] loss: 0.070\n","[3,     5] loss: 0.071\n","[3,     6] loss: 0.073\n","[3,     7] loss: 0.073\n","[3,     8] loss: 0.065\n","[3,     9] loss: 0.068\n","[3,    10] loss: 0.064\n","[3,    11] loss: 0.059\n","[3,    12] loss: 0.064\n","[3,    13] loss: 0.065\n","[3,    14] loss: 0.060\n","[3,    15] loss: 0.060\n","[3,    16] loss: 0.057\n","[3,    17] loss: 0.056\n","[3,    18] loss: 0.058\n","[3,    19] loss: 0.063\n","[3,    20] loss: 0.063\n","correct 245350 / 400000\n","Valid accuracy: 61 %\n","best_val_acc: 61.3375 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[4,     1] loss: 0.056\n","[4,     2] loss: 0.058\n","[4,     3] loss: 0.055\n","[4,     4] loss: 0.060\n","[4,     5] loss: 0.060\n","[4,     6] loss: 0.055\n","[4,     7] loss: 0.058\n","[4,     8] loss: 0.055\n","[4,     9] loss: 0.052\n","[4,    10] loss: 0.053\n","[4,    11] loss: 0.055\n","[4,    12] loss: 0.055\n","[4,    13] loss: 0.056\n","[4,    14] loss: 0.054\n","[4,    15] loss: 0.059\n","[4,    16] loss: 0.056\n","[4,    17] loss: 0.061\n","[4,    18] loss: 0.050\n","[4,    19] loss: 0.051\n","[4,    20] loss: 0.059\n","correct 302016 / 400000\n","Valid accuracy: 75 %\n","best_val_acc: 75.504 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[5,     1] loss: 0.053\n","[5,     2] loss: 0.053\n","[5,     3] loss: 0.055\n","[5,     4] loss: 0.051\n","[5,     5] loss: 0.052\n","[5,     6] loss: 0.051\n","[5,     7] loss: 0.050\n","[5,     8] loss: 0.049\n","[5,     9] loss: 0.058\n","[5,    10] loss: 0.050\n","[5,    11] loss: 0.061\n","[5,    12] loss: 0.062\n","[5,    13] loss: 0.051\n","[5,    14] loss: 0.054\n","[5,    15] loss: 0.050\n","[5,    16] loss: 0.053\n","[5,    17] loss: 0.048\n","[5,    18] loss: 0.050\n","[5,    19] loss: 0.048\n","[5,    20] loss: 0.046\n","correct 336952 / 400000\n","Valid accuracy: 84 %\n","best_val_acc: 84.238 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[6,     1] loss: 0.045\n","[6,     2] loss: 0.043\n","[6,     3] loss: 0.044\n","[6,     4] loss: 0.048\n","[6,     5] loss: 0.046\n","[6,     6] loss: 0.045\n","[6,     7] loss: 0.048\n","[6,     8] loss: 0.040\n","[6,     9] loss: 0.044\n","[6,    10] loss: 0.041\n","[6,    11] loss: 0.043\n","[6,    12] loss: 0.056\n","[6,    13] loss: 0.046\n","[6,    14] loss: 0.044\n","[6,    15] loss: 0.042\n","[6,    16] loss: 0.045\n","[6,    17] loss: 0.040\n","[6,    18] loss: 0.049\n","[6,    19] loss: 0.044\n","[6,    20] loss: 0.046\n","correct 341105 / 400000\n","Valid accuracy: 85 %\n","best_val_acc: 85.27625 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[7,     1] loss: 0.045\n","[7,     2] loss: 0.043\n","[7,     3] loss: 0.047\n","[7,     4] loss: 0.043\n","[7,     5] loss: 0.045\n","[7,     6] loss: 0.041\n","[7,     7] loss: 0.046\n","[7,     8] loss: 0.040\n","[7,     9] loss: 0.043\n","[7,    10] loss: 0.040\n","[7,    11] loss: 0.043\n","[7,    12] loss: 0.040\n","[7,    13] loss: 0.044\n","[7,    14] loss: 0.040\n","[7,    15] loss: 0.054\n","[7,    16] loss: 0.040\n","[7,    17] loss: 0.047\n","[7,    18] loss: 0.042\n","[7,    19] loss: 0.041\n","[7,    20] loss: 0.040\n","correct 352252 / 400000\n","Valid accuracy: 88 %\n","best_val_acc: 88.063 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[8,     1] loss: 0.044\n","[8,     2] loss: 0.046\n","[8,     3] loss: 0.052\n","[8,     4] loss: 0.042\n","[8,     5] loss: 0.039\n","[8,     6] loss: 0.044\n","[8,     7] loss: 0.038\n","[8,     8] loss: 0.040\n","[8,     9] loss: 0.044\n","[8,    10] loss: 0.039\n","[8,    11] loss: 0.039\n","[8,    12] loss: 0.038\n","[8,    13] loss: 0.038\n","[8,    14] loss: 0.041\n","[8,    15] loss: 0.045\n","[8,    16] loss: 0.039\n","[8,    17] loss: 0.039\n","[8,    18] loss: 0.033\n","[8,    19] loss: 0.039\n","[8,    20] loss: 0.043\n","correct 367018 / 400000\n","Valid accuracy: 91 %\n","best_val_acc: 91.7545 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[9,     1] loss: 0.039\n","[9,     2] loss: 0.042\n","[9,     3] loss: 0.034\n","[9,     4] loss: 0.034\n","[9,     5] loss: 0.038\n","[9,     6] loss: 0.039\n","[9,     7] loss: 0.034\n","[9,     8] loss: 0.040\n","[9,     9] loss: 0.035\n","[9,    10] loss: 0.047\n","[9,    11] loss: 0.040\n","[9,    12] loss: 0.040\n","[9,    13] loss: 0.040\n","[9,    14] loss: 0.042\n","[9,    15] loss: 0.039\n","[9,    16] loss: 0.039\n","[9,    17] loss: 0.040\n","[9,    18] loss: 0.037\n","[9,    19] loss: 0.040\n","[9,    20] loss: 0.041\n","correct 321106 / 400000\n","Valid accuracy: 80 %\n","[10,     1] loss: 0.042\n","[10,     2] loss: 0.044\n","[10,     3] loss: 0.040\n","[10,     4] loss: 0.034\n","[10,     5] loss: 0.039\n","[10,     6] loss: 0.034\n","[10,     7] loss: 0.038\n","[10,     8] loss: 0.038\n","[10,     9] loss: 0.037\n","[10,    10] loss: 0.039\n","[10,    11] loss: 0.033\n","[10,    12] loss: 0.036\n","[10,    13] loss: 0.032\n","[10,    14] loss: 0.033\n","[10,    15] loss: 0.034\n","[10,    16] loss: 0.033\n","[10,    17] loss: 0.039\n","[10,    18] loss: 0.038\n","[10,    19] loss: 0.033\n","[10,    20] loss: 0.033\n","correct 344292 / 400000\n","Valid accuracy: 86 %\n","[11,     1] loss: 0.033\n","[11,     2] loss: 0.037\n","[11,     3] loss: 0.043\n","[11,     4] loss: 0.037\n","[11,     5] loss: 0.035\n","[11,     6] loss: 0.036\n","[11,     7] loss: 0.036\n","[11,     8] loss: 0.053\n","[11,     9] loss: 0.060\n","[11,    10] loss: 0.033\n","[11,    11] loss: 0.043\n","[11,    12] loss: 0.050\n","[11,    13] loss: 0.042\n","[11,    14] loss: 0.044\n","[11,    15] loss: 0.042\n","[11,    16] loss: 0.050\n","[11,    17] loss: 0.054\n","[11,    18] loss: 0.051\n","[11,    19] loss: 0.050\n","[11,    20] loss: 0.049\n","correct 249411 / 400000\n","Valid accuracy: 62 %\n","[12,     1] loss: 0.053\n","[12,     2] loss: 0.045\n","[12,     3] loss: 0.052\n","[12,     4] loss: 0.044\n","[12,     5] loss: 0.045\n","[12,     6] loss: 0.040\n","[12,     7] loss: 0.046\n","[12,     8] loss: 0.045\n","[12,     9] loss: 0.041\n","[12,    10] loss: 0.040\n","[12,    11] loss: 0.039\n","[12,    12] loss: 0.047\n","[12,    13] loss: 0.040\n","[12,    14] loss: 0.036\n","[12,    15] loss: 0.036\n","[12,    16] loss: 0.035\n","[12,    17] loss: 0.046\n","[12,    18] loss: 0.037\n","[12,    19] loss: 0.038\n","[12,    20] loss: 0.040\n","correct 344368 / 400000\n","Valid accuracy: 86 %\n","[13,     1] loss: 0.040\n","[13,     2] loss: 0.042\n","[13,     3] loss: 0.040\n","[13,     4] loss: 0.038\n","[13,     5] loss: 0.031\n","[13,     6] loss: 0.042\n","[13,     7] loss: 0.035\n","[13,     8] loss: 0.037\n","[13,     9] loss: 0.042\n","[13,    10] loss: 0.040\n","[13,    11] loss: 0.040\n","[13,    12] loss: 0.038\n","[13,    13] loss: 0.039\n","[13,    14] loss: 0.038\n","[13,    15] loss: 0.040\n","[13,    16] loss: 0.037\n","[13,    17] loss: 0.041\n","[13,    18] loss: 0.039\n","[13,    19] loss: 0.044\n","[13,    20] loss: 0.037\n","correct 350913 / 400000\n","Valid accuracy: 87 %\n","[14,     1] loss: 0.038\n","[14,     2] loss: 0.036\n","[14,     3] loss: 0.038\n","[14,     4] loss: 0.045\n","[14,     5] loss: 0.041\n","[14,     6] loss: 0.035\n","[14,     7] loss: 0.042\n","[14,     8] loss: 0.043\n","[14,     9] loss: 0.040\n","[14,    10] loss: 0.033\n","[14,    11] loss: 0.036\n","[14,    12] loss: 0.036\n","[14,    13] loss: 0.036\n","[14,    14] loss: 0.041\n","[14,    15] loss: 0.038\n","[14,    16] loss: 0.036\n","[14,    17] loss: 0.037\n","[14,    18] loss: 0.033\n","[14,    19] loss: 0.038\n","[14,    20] loss: 0.034\n","correct 340458 / 400000\n","Valid accuracy: 85 %\n","[15,     1] loss: 0.032\n","[15,     2] loss: 0.030\n","[15,     3] loss: 0.032\n","[15,     4] loss: 0.032\n","[15,     5] loss: 0.028\n","[15,     6] loss: 0.038\n","[15,     7] loss: 0.030\n","[15,     8] loss: 0.028\n","[15,     9] loss: 0.039\n","[15,    10] loss: 0.039\n","[15,    11] loss: 0.036\n","[15,    12] loss: 0.033\n","[15,    13] loss: 0.034\n","[15,    14] loss: 0.035\n","[15,    15] loss: 0.034\n","[15,    16] loss: 0.030\n","[15,    17] loss: 0.035\n","[15,    18] loss: 0.031\n","[15,    19] loss: 0.029\n","[15,    20] loss: 0.036\n","correct 308611 / 400000\n","Valid accuracy: 77 %\n"]}],"source":["train(pointnet, train_loader, val_loader, save=True)"]},{"cell_type":"markdown","metadata":{"id":"bvro12zO6PMY"},"source":["## Test\n","\n","\n","\n"]},{"cell_type":"code","execution_count":48,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1655804261330,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"RVHtPNB76PMb","outputId":"14e6c43f-b555-48fa-d7b0-d69b9d1acad3"},"outputs":[{"data":{"text/plain":["PointNetSeg(\n","  (pointnet): PointNet(\n","    (input_transform): TNet(\n","      (mlp1): MLP(\n","        (conv): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp2): MLP(\n","        (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp3): MLP(\n","        (conv): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn1): FC_BN(\n","        (lin): Linear(in_features=1024, out_features=512, bias=True)\n","        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn2): FC_BN(\n","        (lin): Linear(in_features=512, out_features=256, bias=True)\n","        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc3): Linear(in_features=256, out_features=9, bias=True)\n","    )\n","    (mlp1_1): MLP(\n","      (conv): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp1_2): MLP(\n","      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (feature_transform): TNet(\n","      (mlp1): MLP(\n","        (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp2): MLP(\n","        (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp3): MLP(\n","        (conv): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn1): FC_BN(\n","        (lin): Linear(in_features=1024, out_features=512, bias=True)\n","        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn2): FC_BN(\n","        (lin): Linear(in_features=512, out_features=256, bias=True)\n","        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc3): Linear(in_features=256, out_features=4096, bias=True)\n","    )\n","    (mlp2_1): MLP(\n","      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_2): MLP(\n","      (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_3): MLP(\n","      (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_4): MLP(\n","      (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_5): MLP(\n","      (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (mlp1_1): MLP(\n","    (conv): Conv1d(1088, 512, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp1_2): MLP(\n","    (conv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp1_3): MLP(\n","    (conv): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp2_1): MLP(\n","    (conv): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp2_2): MLP(\n","    (conv): Conv1d(128, 3, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (logsoftmax): LogSoftmax(dim=1)\n",")"]},"execution_count":48,"metadata":{},"output_type":"execute_result"}],"source":["# create a new instantiation of PointNetSeg model\n","pointnet = PointNetSeg()\n","\n","# load pyTorch model weights\n","model_path = os.path.join(drive_path, \"MyDrive\", \"pointnetmodel.yml\")\n","pointnet.load_state_dict(torch.load(model_path))\n","\n","# move the model to cuda\n","pointnet.to(device)"]},{"cell_type":"code","execution_count":49,"metadata":{"cellView":"form","executionInfo":{"elapsed":9,"status":"ok","timestamp":1655804261331,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"koCvhQAk6PMb"},"outputs":[],"source":["def compute_stats(true_labels, pred_labels):\n","  unk     = np.count_nonzero(true_labels == 0)\n","  trav    = np.count_nonzero(true_labels == 1)\n","  nontrav = np.count_nonzero(true_labels == 2)\n","\n","  total_predictions = labels.shape[1]*labels.shape[0]\n","  correct = (true_labels == pred_labels).sum().item()\n","\n","  return correct, total_predictions"]},{"cell_type":"code","execution_count":50,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13669,"status":"ok","timestamp":1655804274992,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"pvxImU226PMc","outputId":"7150aa7b-ef49-4f16-8e0f-851b4f276641"},"outputs":[{"name":"stderr","output_type":"stream","text":["30it [00:13,  2.28it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Test accuracy: 88.97766666666666 %\n","total time: 13.170599937438965  [s]\n","avg time  : 0.43901999791463214  [s]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["pointnet.eval()\n","total_correct_predictions = total_predictions = 0\n","\n","start = time.time()\n","\n","for i, data in tqdm(enumerate(test_loader, 0)):\n","  inputs, labels = data\n","  inputs = inputs.to(device).float()\n","  labels = labels.to(device)\n","  outputs, __, __ = pointnet(inputs.transpose(1,2))  \n","  _, predicted = torch.max(outputs.data, 1)\n","\n","  # visualize results\n","  remapped_pred = remap_to_bgr(predicted[0].cpu().numpy(), remap_color_scheme)\n","  np_pointcloud = inputs[0].cpu().numpy()\n","  # visualize3DPointCloud(np_pointcloud, remapped_pred)\n","  \n","  # compute statistics\n","  ground_truth_labels = labels.cpu()\n","  predicted_labels    = predicted.cpu()\n","  correct, total = compute_stats(ground_truth_labels, predicted_labels)\n","\n","  total_correct_predictions += correct\n","  total_predictions         += total\n","\n","end = time.time()\n","\n","# nice layout after tqdm\n","print()\n","print()\n","\n","test_acc    = 100. * total_correct_predictions / total_predictions\n","tot_latency = end-start\n","avg_latency = tot_latency / len(test_loader.dataset)\n","\n","print('Test accuracy:', test_acc, \"%\")\n","print('total time:',    tot_latency, \" [s]\")\n","print('avg time  :',    avg_latency, \" [s]\")\n"]},{"cell_type":"markdown","metadata":{"id":"CyWPmYda6A5Q"},"source":["# Experiment 3\n","\n","The two previous experiment are combined together."]},{"cell_type":"markdown","metadata":{"id":"g-GBmZUw6QAE"},"source":["## Network redefinition"]},{"cell_type":"markdown","metadata":{"id":"XnNKxbwM6QAF"},"source":["### PointNet Module"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1655804274993,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"cZwm6PUF7QaQ"},"outputs":[],"source":["class PointNet(nn.Module):\n","   def __init__(self):\n","        super().__init__()\n","        self.input_transform   = TNet(k=3)\n","        \n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        self.mlp1_1 = MLP(3, 64)\n","        self.mlp1_2 = MLP(64, 64)\n","\n","        self.feature_transform = TNet(k=64)\n","        \n","        self.mlp2_1 = MLP(64, 64)\n","        self.mlp2_2 = MLP(64, 128)\n","        self.mlp2_3 = MLP(128, 256)\n","        self.mlp2_4 = MLP(256, 512)\n","        self.mlp2_5 = MLP(512, 1024)\n","\n","        ###########################################################\n","\n","\n","   def forward(self, input):\n","        n_pts = input.size()[2]\n","        matrix3x3 = self.input_transform(input)\n","        input_transform_output = torch.bmm(torch.transpose(input,1,2), matrix3x3).transpose(1,2)\n","\n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        xb = self.mlp1_1(input_transform_output)\n","        xb = self.mlp1_2(xb)\n","\n","        #FEATURE TRANSFORM PART\n","        #computation feature transform by using T-Net\n","        matrix64x64 = self.feature_transform(xb)\n","        #matrix multiplication among features and feature transform matrix\n","        feature_transform_output = torch.bmm(torch.transpose(xb,1,2), matrix64x64).transpose(1,2)\n","\n","        xb = self.mlp2_1(feature_transform_output)\n","        xb = self.mlp2_2(xb)\n","        xb = self.mlp2_3(xb)\n","        xb = self.mlp2_4(xb)\n","        xb = self.mlp2_5(xb) \n","\n","        pool = nn.MaxPool1d(xb.size(-1))(xb)\n","        global_feature = nn.Flatten(1)(pool)\n","        \n","        ###########################################################\n","\n","        global_feature_repeated = nn.Flatten(1)(global_feature).repeat(n_pts,1,1).transpose(0,2).transpose(0,1)\n","\n","        return [feature_transform_output, global_feature_repeated], matrix3x3, matrix64x64"]},{"cell_type":"markdown","metadata":{"id":"fsYGAe9W6QAF"},"source":["### PointNetSeg Module"]},{"cell_type":"code","execution_count":52,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1655804274993,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"LhqxmMoh7M4i"},"outputs":[],"source":["class PointNetSeg(nn.Module):\n","    def __init__(self, classes = 3):\n","        super().__init__()\n","        self.pointnet = PointNet()\n","\n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","        \n","        self.mlp1_1 = MLP(1088,512)\n","        self.mlp1_2 = MLP(512,256)\n","        self.mlp1_3 = MLP(256,128)\n","\n","        self.mlp2 = MLP(128,classes)\n","\n","        ###########################################################\n","\n","        self.logsoftmax = nn.LogSoftmax(dim=1)\n","        \n","\n","    def forward(self, input):\n","        inputs, matrix3x3, matrix64x64 = self.pointnet(input)\n","        stack = torch.cat(inputs,1)\n","        \n","        ###########################################################\n","        ################## INSERT YOUR CODE HERE ##################\n","        ###########################################################\n","\n","        xb = self.mlp1_1(stack)\n","        xb = self.mlp1_2(xb)\n","        point_features = self.mlp1_3(xb)\n","        \n","        output = self.mlp2(point_features)\n","\n","        ###########################################################\n","\n","        return self.logsoftmax(output), matrix3x3, matrix64x64"]},{"cell_type":"markdown","metadata":{"id":"YP_7SAu26QAF"},"source":["## Training loop"]},{"cell_type":"code","execution_count":53,"metadata":{"cellView":"form","executionInfo":{"elapsed":7,"status":"ok","timestamp":1655804274994,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"8t5TzUbG6QAF"},"outputs":[],"source":["pointnet = PointNetSeg()\n","pointnet.to(device);"]},{"cell_type":"code","execution_count":54,"metadata":{"cellView":"form","executionInfo":{"elapsed":7,"status":"ok","timestamp":1655804274994,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"E3iOXCw_6QAF"},"outputs":[],"source":["optimizer = torch.optim.Adam(pointnet.parameters(), lr=0.005)"]},{"cell_type":"code","execution_count":55,"metadata":{"cellView":"form","executionInfo":{"elapsed":6,"status":"ok","timestamp":1655804274994,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"Ab-iFQrv6QAG"},"outputs":[],"source":["def train(model, train_loader, val_loader=None,  epochs=15, save=True):\n","    best_val_acc = -1.0\n","    for epoch in range(epochs): \n","        pointnet.train()\n","        running_loss = 0.0\n","\n","        for i, data in enumerate(train_loader, 0):\n","            inputs, labels = data\n","            inputs = inputs.to(device).float()\n","            labels = labels.to(device)\n","            optimizer.zero_grad()\n","            outputs, m3x3, m64x64 = pointnet(inputs.transpose(1,2))\n","            loss = pointNetLoss(outputs, labels, m3x3, m64x64)\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print statistics\n","            running_loss += loss.item()\n","            if i % 10 == 9 or True:    # print every 10 mini-batches\n","                    print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))\n","                    running_loss = 0.0\n","\n","        pointnet.eval()\n","        correct = total = 0\n","\n","        # validation\n","        with torch.no_grad():\n","            for data in val_loader:\n","                inputs, labels = data\n","                inputs = inputs.to(device).float()\n","                labels = labels.to(device)\n","                outputs, __, __ = pointnet(inputs.transpose(1,2))\n","                _, predicted = torch.max(outputs.data, 1)\n","                \n","                total   += labels.size(0) * labels.size(1)\n","                correct += (predicted == labels).sum().item()\n","\n","        print(\"correct\", correct, \"/\", total)\n","        val_acc = 100.0 * correct / total\n","        print('Valid accuracy: %d %%' % val_acc)\n","\n","        # save the model\n","        if save and val_acc > best_val_acc:\n","            best_val_acc = val_acc\n","            path = os.path.join(drive_path, \"MyDrive\", \"pointnetmodel.yml\")\n","            print(\"best_val_acc:\", val_acc, \"saving model at\", path)\n","            torch.save(pointnet.state_dict(), path)"]},{"cell_type":"code","execution_count":56,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":798173,"status":"ok","timestamp":1655805073161,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"jH2M211P6QAG","outputId":"f43bd283-9774-4724-fbbb-afcf98095825"},"outputs":[{"name":"stdout","output_type":"stream","text":["[1,     1] loss: 0.122\n","[1,     2] loss: 0.126\n","[1,     3] loss: 0.126\n","[1,     4] loss: 0.120\n","[1,     5] loss: 0.115\n","[1,     6] loss: 0.116\n","[1,     7] loss: 0.111\n","[1,     8] loss: 0.114\n","[1,     9] loss: 0.115\n","[1,    10] loss: 0.115\n","[1,    11] loss: 0.113\n","[1,    12] loss: 0.113\n","[1,    13] loss: 0.108\n","[1,    14] loss: 0.111\n","[1,    15] loss: 0.106\n","[1,    16] loss: 0.110\n","[1,    17] loss: 0.105\n","[1,    18] loss: 0.109\n","[1,    19] loss: 0.110\n","[1,    20] loss: 0.104\n","correct 268545 / 400000\n","Valid accuracy: 67 %\n","best_val_acc: 67.13625 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[2,     1] loss: 0.103\n","[2,     2] loss: 0.106\n","[2,     3] loss: 0.105\n","[2,     4] loss: 0.103\n","[2,     5] loss: 0.101\n","[2,     6] loss: 0.097\n","[2,     7] loss: 0.096\n","[2,     8] loss: 0.099\n","[2,     9] loss: 0.096\n","[2,    10] loss: 0.098\n","[2,    11] loss: 0.098\n","[2,    12] loss: 0.094\n","[2,    13] loss: 0.094\n","[2,    14] loss: 0.095\n","[2,    15] loss: 0.096\n","[2,    16] loss: 0.093\n","[2,    17] loss: 0.092\n","[2,    18] loss: 0.090\n","[2,    19] loss: 0.087\n","[2,    20] loss: 0.092\n","correct 285133 / 400000\n","Valid accuracy: 71 %\n","best_val_acc: 71.28325 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[3,     1] loss: 0.092\n","[3,     2] loss: 0.083\n","[3,     3] loss: 0.087\n","[3,     4] loss: 0.084\n","[3,     5] loss: 0.087\n","[3,     6] loss: 0.080\n","[3,     7] loss: 0.087\n","[3,     8] loss: 0.087\n","[3,     9] loss: 0.081\n","[3,    10] loss: 0.081\n","[3,    11] loss: 0.077\n","[3,    12] loss: 0.078\n","[3,    13] loss: 0.079\n","[3,    14] loss: 0.077\n","[3,    15] loss: 0.077\n","[3,    16] loss: 0.082\n","[3,    17] loss: 0.078\n","[3,    18] loss: 0.075\n","[3,    19] loss: 0.077\n","[3,    20] loss: 0.072\n","correct 253032 / 400000\n","Valid accuracy: 63 %\n","[4,     1] loss: 0.073\n","[4,     2] loss: 0.076\n","[4,     3] loss: 0.074\n","[4,     4] loss: 0.077\n","[4,     5] loss: 0.071\n","[4,     6] loss: 0.071\n","[4,     7] loss: 0.073\n","[4,     8] loss: 0.068\n","[4,     9] loss: 0.068\n","[4,    10] loss: 0.071\n","[4,    11] loss: 0.071\n","[4,    12] loss: 0.068\n","[4,    13] loss: 0.069\n","[4,    14] loss: 0.069\n","[4,    15] loss: 0.071\n","[4,    16] loss: 0.069\n","[4,    17] loss: 0.068\n","[4,    18] loss: 0.066\n","[4,    19] loss: 0.065\n","[4,    20] loss: 0.070\n","correct 269713 / 400000\n","Valid accuracy: 67 %\n","[5,     1] loss: 0.065\n","[5,     2] loss: 0.069\n","[5,     3] loss: 0.065\n","[5,     4] loss: 0.064\n","[5,     5] loss: 0.060\n","[5,     6] loss: 0.062\n","[5,     7] loss: 0.067\n","[5,     8] loss: 0.061\n","[5,     9] loss: 0.061\n","[5,    10] loss: 0.061\n","[5,    11] loss: 0.060\n","[5,    12] loss: 0.058\n","[5,    13] loss: 0.060\n","[5,    14] loss: 0.058\n","[5,    15] loss: 0.056\n","[5,    16] loss: 0.052\n","[5,    17] loss: 0.057\n","[5,    18] loss: 0.051\n","[5,    19] loss: 0.052\n","[5,    20] loss: 0.063\n","correct 324169 / 400000\n","Valid accuracy: 81 %\n","best_val_acc: 81.04225 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[6,     1] loss: 0.052\n","[6,     2] loss: 0.053\n","[6,     3] loss: 0.055\n","[6,     4] loss: 0.054\n","[6,     5] loss: 0.050\n","[6,     6] loss: 0.051\n","[6,     7] loss: 0.045\n","[6,     8] loss: 0.048\n","[6,     9] loss: 0.046\n","[6,    10] loss: 0.045\n","[6,    11] loss: 0.043\n","[6,    12] loss: 0.047\n","[6,    13] loss: 0.047\n","[6,    14] loss: 0.046\n","[6,    15] loss: 0.048\n","[6,    16] loss: 0.046\n","[6,    17] loss: 0.049\n","[6,    18] loss: 0.044\n","[6,    19] loss: 0.042\n","[6,    20] loss: 0.047\n","correct 328390 / 400000\n","Valid accuracy: 82 %\n","best_val_acc: 82.0975 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[7,     1] loss: 0.048\n","[7,     2] loss: 0.045\n","[7,     3] loss: 0.046\n","[7,     4] loss: 0.046\n","[7,     5] loss: 0.045\n","[7,     6] loss: 0.049\n","[7,     7] loss: 0.044\n","[7,     8] loss: 0.046\n","[7,     9] loss: 0.047\n","[7,    10] loss: 0.046\n","[7,    11] loss: 0.043\n","[7,    12] loss: 0.040\n","[7,    13] loss: 0.043\n","[7,    14] loss: 0.041\n","[7,    15] loss: 0.047\n","[7,    16] loss: 0.040\n","[7,    17] loss: 0.046\n","[7,    18] loss: 0.043\n","[7,    19] loss: 0.043\n","[7,    20] loss: 0.042\n","correct 366806 / 400000\n","Valid accuracy: 91 %\n","best_val_acc: 91.7015 saving model at /content/drive/MyDrive/pointnetmodel.yml\n","[8,     1] loss: 0.038\n","[8,     2] loss: 0.045\n","[8,     3] loss: 0.044\n","[8,     4] loss: 0.039\n","[8,     5] loss: 0.040\n","[8,     6] loss: 0.041\n","[8,     7] loss: 0.039\n","[8,     8] loss: 0.042\n","[8,     9] loss: 0.043\n","[8,    10] loss: 0.038\n","[8,    11] loss: 0.038\n","[8,    12] loss: 0.046\n","[8,    13] loss: 0.049\n","[8,    14] loss: 0.045\n","[8,    15] loss: 0.037\n","[8,    16] loss: 0.039\n","[8,    17] loss: 0.039\n","[8,    18] loss: 0.041\n","[8,    19] loss: 0.040\n","[8,    20] loss: 0.041\n","correct 257248 / 400000\n","Valid accuracy: 64 %\n","[9,     1] loss: 0.040\n","[9,     2] loss: 0.039\n","[9,     3] loss: 0.046\n","[9,     4] loss: 0.040\n","[9,     5] loss: 0.038\n","[9,     6] loss: 0.037\n","[9,     7] loss: 0.042\n","[9,     8] loss: 0.046\n","[9,     9] loss: 0.045\n","[9,    10] loss: 0.041\n","[9,    11] loss: 0.036\n","[9,    12] loss: 0.038\n","[9,    13] loss: 0.041\n","[9,    14] loss: 0.035\n","[9,    15] loss: 0.033\n","[9,    16] loss: 0.032\n","[9,    17] loss: 0.040\n","[9,    18] loss: 0.034\n","[9,    19] loss: 0.040\n","[9,    20] loss: 0.036\n","correct 363867 / 400000\n","Valid accuracy: 90 %\n","[10,     1] loss: 0.039\n","[10,     2] loss: 0.033\n","[10,     3] loss: 0.034\n","[10,     4] loss: 0.035\n","[10,     5] loss: 0.037\n","[10,     6] loss: 0.033\n","[10,     7] loss: 0.031\n","[10,     8] loss: 0.034\n","[10,     9] loss: 0.039\n","[10,    10] loss: 0.035\n","[10,    11] loss: 0.031\n","[10,    12] loss: 0.030\n","[10,    13] loss: 0.037\n","[10,    14] loss: 0.033\n","[10,    15] loss: 0.030\n","[10,    16] loss: 0.031\n","[10,    17] loss: 0.034\n","[10,    18] loss: 0.033\n","[10,    19] loss: 0.035\n","[10,    20] loss: 0.033\n","correct 364978 / 400000\n","Valid accuracy: 91 %\n","[11,     1] loss: 0.034\n","[11,     2] loss: 0.032\n","[11,     3] loss: 0.033\n","[11,     4] loss: 0.031\n","[11,     5] loss: 0.034\n","[11,     6] loss: 0.040\n","[11,     7] loss: 0.028\n","[11,     8] loss: 0.035\n","[11,     9] loss: 0.035\n","[11,    10] loss: 0.047\n","[11,    11] loss: 0.040\n","[11,    12] loss: 0.036\n","[11,    13] loss: 0.045\n","[11,    14] loss: 0.038\n","[11,    15] loss: 0.037\n","[11,    16] loss: 0.039\n","[11,    17] loss: 0.035\n","[11,    18] loss: 0.039\n","[11,    19] loss: 0.032\n","[11,    20] loss: 0.043\n","correct 352292 / 400000\n","Valid accuracy: 88 %\n","[12,     1] loss: 0.033\n","[12,     2] loss: 0.030\n","[12,     3] loss: 0.033\n","[12,     4] loss: 0.034\n","[12,     5] loss: 0.033\n","[12,     6] loss: 0.030\n","[12,     7] loss: 0.036\n","[12,     8] loss: 0.029\n","[12,     9] loss: 0.032\n","[12,    10] loss: 0.034\n","[12,    11] loss: 0.030\n","[12,    12] loss: 0.031\n","[12,    13] loss: 0.028\n","[12,    14] loss: 0.040\n","[12,    15] loss: 0.034\n","[12,    16] loss: 0.026\n","[12,    17] loss: 0.037\n","[12,    18] loss: 0.034\n","[12,    19] loss: 0.031\n","[12,    20] loss: 0.034\n","correct 309621 / 400000\n","Valid accuracy: 77 %\n","[13,     1] loss: 0.031\n","[13,     2] loss: 0.032\n","[13,     3] loss: 0.036\n","[13,     4] loss: 0.032\n","[13,     5] loss: 0.031\n","[13,     6] loss: 0.029\n","[13,     7] loss: 0.028\n","[13,     8] loss: 0.031\n","[13,     9] loss: 0.031\n","[13,    10] loss: 0.036\n","[13,    11] loss: 0.032\n","[13,    12] loss: 0.029\n","[13,    13] loss: 0.041\n","[13,    14] loss: 0.030\n","[13,    15] loss: 0.038\n","[13,    16] loss: 0.033\n","[13,    17] loss: 0.029\n","[13,    18] loss: 0.031\n","[13,    19] loss: 0.043\n","[13,    20] loss: 0.033\n","correct 300298 / 400000\n","Valid accuracy: 75 %\n","[14,     1] loss: 0.031\n","[14,     2] loss: 0.030\n","[14,     3] loss: 0.028\n","[14,     4] loss: 0.037\n","[14,     5] loss: 0.036\n","[14,     6] loss: 0.033\n","[14,     7] loss: 0.032\n","[14,     8] loss: 0.026\n","[14,     9] loss: 0.028\n","[14,    10] loss: 0.035\n","[14,    11] loss: 0.026\n","[14,    12] loss: 0.032\n","[14,    13] loss: 0.030\n","[14,    14] loss: 0.032\n","[14,    15] loss: 0.034\n","[14,    16] loss: 0.036\n","[14,    17] loss: 0.032\n","[14,    18] loss: 0.031\n","[14,    19] loss: 0.030\n","[14,    20] loss: 0.030\n","correct 265809 / 400000\n","Valid accuracy: 66 %\n","[15,     1] loss: 0.024\n","[15,     2] loss: 0.028\n","[15,     3] loss: 0.032\n","[15,     4] loss: 0.035\n","[15,     5] loss: 0.035\n","[15,     6] loss: 0.029\n","[15,     7] loss: 0.031\n","[15,     8] loss: 0.028\n","[15,     9] loss: 0.032\n","[15,    10] loss: 0.032\n","[15,    11] loss: 0.029\n","[15,    12] loss: 0.031\n","[15,    13] loss: 0.024\n","[15,    14] loss: 0.032\n","[15,    15] loss: 0.034\n","[15,    16] loss: 0.028\n","[15,    17] loss: 0.028\n","[15,    18] loss: 0.027\n","[15,    19] loss: 0.027\n","[15,    20] loss: 0.032\n","correct 319612 / 400000\n","Valid accuracy: 79 %\n"]}],"source":["train(pointnet, train_loader, val_loader, save=True)"]},{"cell_type":"markdown","metadata":{"id":"tgVTgXZ_6QAG"},"source":["## Test\n","\n","\n","\n"]},{"cell_type":"code","execution_count":57,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1655805073162,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"9S-9WGsv6QAG","outputId":"0bda6c94-a48a-4fff-faa5-52d3e6a65918"},"outputs":[{"data":{"text/plain":["PointNetSeg(\n","  (pointnet): PointNet(\n","    (input_transform): TNet(\n","      (mlp1): MLP(\n","        (conv): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp2): MLP(\n","        (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp3): MLP(\n","        (conv): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn1): FC_BN(\n","        (lin): Linear(in_features=1024, out_features=512, bias=True)\n","        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn2): FC_BN(\n","        (lin): Linear(in_features=512, out_features=256, bias=True)\n","        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc3): Linear(in_features=256, out_features=9, bias=True)\n","    )\n","    (mlp1_1): MLP(\n","      (conv): Conv1d(3, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp1_2): MLP(\n","      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (feature_transform): TNet(\n","      (mlp1): MLP(\n","        (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp2): MLP(\n","        (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (mlp3): MLP(\n","        (conv): Conv1d(128, 1024, kernel_size=(1,), stride=(1,))\n","        (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn1): FC_BN(\n","        (lin): Linear(in_features=1024, out_features=512, bias=True)\n","        (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc_bn2): FC_BN(\n","        (lin): Linear(in_features=512, out_features=256, bias=True)\n","        (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","      (fc3): Linear(in_features=256, out_features=4096, bias=True)\n","    )\n","    (mlp2_1): MLP(\n","      (conv): Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_2): MLP(\n","      (conv): Conv1d(64, 128, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_3): MLP(\n","      (conv): Conv1d(128, 256, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_4): MLP(\n","      (conv): Conv1d(256, 512, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (mlp2_5): MLP(\n","      (conv): Conv1d(512, 1024, kernel_size=(1,), stride=(1,))\n","      (bn): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (mlp1_1): MLP(\n","    (conv): Conv1d(1088, 512, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp1_2): MLP(\n","    (conv): Conv1d(512, 256, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp1_3): MLP(\n","    (conv): Conv1d(256, 128, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (mlp2): MLP(\n","    (conv): Conv1d(128, 3, kernel_size=(1,), stride=(1,))\n","    (bn): BatchNorm1d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  )\n","  (logsoftmax): LogSoftmax(dim=1)\n",")"]},"execution_count":57,"metadata":{},"output_type":"execute_result"}],"source":["# create a new instantiation of PointNetSeg model\n","pointnet = PointNetSeg()\n","\n","# load pyTorch model weights\n","model_path = os.path.join(drive_path, \"MyDrive\", \"pointnetmodel.yml\")\n","pointnet.load_state_dict(torch.load(model_path))\n","\n","# move the model to cuda\n","pointnet.to(device)"]},{"cell_type":"code","execution_count":58,"metadata":{"cellView":"form","executionInfo":{"elapsed":8,"status":"ok","timestamp":1655805073162,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"0v4PK75V6QAG"},"outputs":[],"source":["def compute_stats(true_labels, pred_labels):\n","  unk     = np.count_nonzero(true_labels == 0)\n","  trav    = np.count_nonzero(true_labels == 1)\n","  nontrav = np.count_nonzero(true_labels == 2)\n","\n","  total_predictions = labels.shape[1]*labels.shape[0]\n","  correct = (true_labels == pred_labels).sum().item()\n","\n","  return correct, total_predictions"]},{"cell_type":"code","execution_count":59,"metadata":{"cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13457,"status":"ok","timestamp":1655805086612,"user":{"displayName":"Simone Boscolo","userId":"03599200767989993134"},"user_tz":-120},"id":"6dA6YULD6QAH","outputId":"2e553eb8-9741-4c44-c4f0-5a1632691253"},"outputs":[{"name":"stderr","output_type":"stream","text":["30it [00:13,  2.25it/s]"]},{"name":"stdout","output_type":"stream","text":["\n","\n","Test accuracy: 92.9485 %\n","total time: 13.365534782409668  [s]\n","avg time  : 0.44551782608032225  [s]\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["pointnet.eval()\n","total_correct_predictions = total_predictions = 0\n","\n","start = time.time()\n","\n","for i, data in tqdm(enumerate(test_loader, 0)):\n","  inputs, labels = data\n","  inputs = inputs.to(device).float()\n","  labels = labels.to(device)\n","  outputs, __, __ = pointnet(inputs.transpose(1,2))  \n","  _, predicted = torch.max(outputs.data, 1)\n","\n","  # visualize results\n","  remapped_pred = remap_to_bgr(predicted[0].cpu().numpy(), remap_color_scheme)\n","  np_pointcloud = inputs[0].cpu().numpy()\n","  # visualize3DPointCloud(np_pointcloud, remapped_pred)\n","  \n","  # compute statistics\n","  ground_truth_labels = labels.cpu()\n","  predicted_labels    = predicted.cpu()\n","  correct, total = compute_stats(ground_truth_labels, predicted_labels)\n","\n","  total_correct_predictions += correct\n","  total_predictions         += total\n","\n","end = time.time()\n","\n","# nice layout after tqdm\n","print()\n","print()\n","\n","test_acc    = 100. * total_correct_predictions / total_predictions\n","tot_latency = end-start\n","avg_latency = tot_latency / len(test_loader.dataset)\n","\n","print('Test accuracy:', test_acc, \"%\")\n","print('total time:',    tot_latency, \" [s]\")\n","print('avg time  :',    avg_latency, \" [s]\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"3DP_Lab4_Point_Cloud_Segmentation.ipynb","provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
